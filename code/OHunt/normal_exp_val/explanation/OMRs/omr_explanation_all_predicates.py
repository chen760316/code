import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from scipy.stats import zscore
import shap
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('TkAgg')
from sklearn.model_selection import GridSearchCV
from scipy.stats import skew
from sklearn.metrics import log_loss, mean_squared_error
from sklearn.ensemble import IsolationForest
# å±è”½ sklearn çš„æ‰€æœ‰ warningï¼ˆåŒ…æ‹¬ç‰¹å¾åä¸åŒ¹é…çš„ï¼‰
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
import torch
from deepod.models import DeepSAD, RoSAS, PReNet
from deepod.models import REPEN, SLAD, ICL, NeuTraL
from torch.utils.data import TensorDataset, DataLoader

# === 1. åŠ è½½æ•°æ® ===
file_path = "../../datasets/real_outlier/Cardiotocography.csv"
# file_path = "../../datasets/real_outlier/annthyroid.csv"
# file_path = "../../datasets/real_outlier/optdigits.csv"
# file_path = "../../datasets/real_outlier/PageBlocks.csv"
# file_path = "../../datasets/real_outlier/pendigits.csv"
# file_path = "../../datasets/real_outlier/satellite.csv"
# file_path = "../../datasets/real_outlier/shuttle.csv"
# file_path = "../../datasets/real_outlier/yeast.csv"
data = pd.read_csv(file_path)

X = data.iloc[:, :-1]
y = data.iloc[:, -1]

# === 2. æ‹†åˆ†è®­ç»ƒ / æµ‹è¯• / éªŒè¯ ===
original_indices = np.arange(len(X))
# æŒ‰ç…§ 70% è®­ç»ƒé›†ï¼Œ20% éªŒè¯é›†ï¼Œ10% æµ‹è¯•é›†çš„æ¯”ä¾‹éšæœºåˆ’åˆ†æ•°æ®é›†
X_train, X_temp, y_train, y_temp, train_indices, temp_indices = train_test_split(X, y, original_indices, test_size=0.3, random_state=1)
# ä»ä¸´æ—¶æ•°æ®é›†ï¼ˆ30%ï¼‰ä¸­åˆ’åˆ†å‡º 10% æµ‹è¯•é›†å’Œ 20% éªŒè¯é›†
X_val, X_test, y_val, y_test, val_indices, test_indices = train_test_split(X_temp, y_temp, temp_indices, test_size=0.33, random_state=1)

# === 3. è®­ç»ƒ XGBoost æ¨¡å‹ ===
# å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
param_grid = {
    'n_estimators': [50, 100, 150],      # æ ‘çš„æ•°é‡
    'max_depth': [3, 6, 9],              # æ ‘çš„æœ€å¤§æ·±åº¦
    'learning_rate': [0.01, 0.1, 0.2],   # å­¦ä¹ ç‡
    'subsample': [0.8, 1.0],             # å­æ ·æœ¬æ¯”ä¾‹
    'colsample_bytree': [0.8, 1.0],      # æ¯æ£µæ ‘çš„ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
    'scale_pos_weight': [1, 2],          # ç±»åˆ«ä¸å¹³è¡¡æ—¶è°ƒèŠ‚
    'class_weight': [None]               # å…¼å®¹åŸæ ¼å¼ï¼ŒXGBoost å®é™…ä¸æ”¯æŒ class_weightï¼ˆå ä½ï¼‰
}

# åˆå§‹åŒ– XGBoost æ¨¡å‹ï¼ˆä¸ç”¨æŒ‡å®š n_estimators ä¹‹ç±»çš„ï¼Œè¿™äº›ä¼šç”± GridSearch å†³å®šï¼‰
model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    booster='gbtree',
    random_state=42,
    verbosity=0
)

# åˆå§‹åŒ– GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           scoring='accuracy',
                           cv=2,
                           verbose=2,
                           n_jobs=-1)

# æ‹Ÿåˆæ¨¡å‹
grid_search.fit(X_val, y_val)

# è¾“å‡ºæœ€ä½³è¶…å‚æ•°å’Œäº¤å‰éªŒè¯å¾—åˆ†
print("æœ€ä½³è¶…å‚æ•°ç»„åˆï¼š", grid_search.best_params_)
print("æœ€ä½³äº¤å‰éªŒè¯å‡†ç¡®åº¦ï¼š", grid_search.best_score_)

# è·å–æœ€ä½³æ¨¡å‹
model = grid_search.best_estimator_
model.fit(X_train, y_train)

# === 4. é¢„æµ‹ & é€‰å–æŸæ¡è¢«è¯†åˆ«ä¸ºç¦»ç¾¤å€¼çš„æ ·æœ¬ ===
y_pred = model.predict(X_test)
outlier_idx = np.where(y_pred == 1)[0][0]  # ç¬¬ä¸€ä¸ªé¢„æµ‹ä¸ºç¦»ç¾¤å€¼çš„æ ·æœ¬
u = X_test.iloc[outlier_idx]
print("ç›®æ ‡æ ·æœ¬ï¼š\n", u)

# === 5. MDetI(): æ¨¡å‹é‡è¦ç‰¹å¾è§£é‡Š ===
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)

# å±•ç¤ºç›®æ ‡æ ·æœ¬çš„é‡è¦ç‰¹å¾
shap.plots.waterfall(shap_values[outlier_idx], max_display=10)

# æå–ç‰¹å¾é‡è¦æ€§
important_features = shap_values[outlier_idx].values
feature_names = X.columns

# === 5. æ¨¡å‹è§£é‡Šï¼šæ‰¾å‡ºé‡è¦ç‰¹å¾ï¼ˆMDetIï¼‰ ===
important_features_dict = {}  # å­˜å‚¨é‡è¦ç‰¹å¾åŠå…¶ SHAP å€¼

print("\næ¨¡å‹è®¤ä¸ºçš„é‡è¦ç‰¹å¾ï¼ˆMDetIï¼‰:")
for name, value in zip(feature_names, important_features):
    if abs(value) > 0.1:  # ä½ è®¾å®šçš„é˜ˆå€¼
        important_features_dict[name] = value
        print(f"  {name}: SHAP = {value:.3f}")

# choice NeuTraLå¼‚å¸¸æ£€æµ‹å™¨
epochs = 1
device = 'cuda' if torch.cuda.is_available() else 'cpu'
n_trans = 64
random_state = 42
out_clf = NeuTraL(epochs=1, device=device)
# è½¬æ¢ä¸º float32 çš„ numpy æ•°ç»„
X_tensor = torch.tensor(X_train.values, dtype=torch.float32)
out_clf.fit(X_tensor)

# === 6. åªåœ¨é‡è¦ç‰¹å¾ä¸­åš Outlier æ£€æµ‹ï¼ˆZ-score æ–¹æ³•ï¼‰ ===
def is_outlier_zscore(val, col, threshold=1):
    z = (val - col.mean()) / col.std()
    return abs(z) > threshold, z

# === 6. ç¦»ç¾¤å€¼æ£€æµ‹ï¼šåªæ£€æµ‹é‡è¦ç‰¹å¾ ===
outlier_features = {}  # å­˜å‚¨è¢«åˆ¤å®šä¸ºç¦»ç¾¤å€¼çš„ç‰¹å¾åŠå…¶ z-score
print("\nOutlier æ£€æµ‹ï¼ˆä»…é™é‡è¦ç‰¹å¾ï¼‰:")
for f in important_features_dict:
    col_vals = X_train[f]
    is_out, z = is_outlier_zscore(u[f], col_vals)
    if is_out:
        outlier_features[f] = z
        print(f"  {f}: {u[f]} (z-score = {z:.2f}) â†’ æ˜¯ç¦»ç¾¤å€¼")
    else:
        print(f"  {f}: {u[f]} (z-score = {z:.2f}) â†’ æ­£å¸¸")

print("\nImbalanced æ£€æµ‹ï¼ˆä»…å¯¹è¢«åˆ¤ä¸ºç¦»ç¾¤å€¼çš„ç‰¹å¾ï¼‰:")

for f in outlier_features:  # åªå¯¹è¢«åˆ¤ä¸ºç¦»ç¾¤çš„ç‰¹å¾æ£€æµ‹åˆ†å¸ƒå˜åŒ–
    S = X_train[f]  # åŸå§‹è®­ç»ƒé›†ä¸­çš„æŸåˆ—
    S_plus_u = pd.concat([S, pd.Series(u[f])], ignore_index=True)  # åŠ å…¥å½“å‰æ ·æœ¬å€¼

    # === æ ‡å‡†å·®å˜åŒ–
    orig_std = S.std()
    new_std = S_plus_u.std()
    delta_std = abs(new_std - orig_std) / (orig_std + 1e-8)

    # === ååº¦å˜åŒ–ï¼ˆè¡¡é‡åˆ†å¸ƒå¯¹ç§°æ€§ï¼‰
    orig_skew = skew(S)
    new_skew = skew(S_plus_u)
    delta_skew = abs(new_skew - orig_skew)

    # === IQR æ£€æŸ¥ï¼ˆè¡¡é‡åˆ†å¸ƒå°¾éƒ¨å¼‚å¸¸å€¼ï¼‰
    Q1 = S.quantile(0.25)
    Q3 = S.quantile(0.75)
    IQR = Q3 - Q1

    # === åˆ¤æ–­æ˜¯å¦ä¸å¹³è¡¡
    is_imbalanced = False

    if delta_std > 0.02:
        print(f"  {f}ï¼šæ ‡å‡†å·®å˜åŒ– Î” = {delta_std:.2%}")
        is_imbalanced = True
    elif delta_skew > 0.1:
        print(f"  {f}ï¼šååº¦å˜åŒ– Î” = {delta_skew:.3f}")
        is_imbalanced = True
    elif u[f] < Q1 - 1.5 * IQR or u[f] > Q3 + 1.5 * IQR:
        print(f"  {f}ï¼šå€¼ {u[f]} è¶…å‡º IQR èŒƒå›´ ({Q1:.2f}, {Q3:.2f})")
        is_imbalanced = True

    # === è¾“å‡ºä¸å¹³è¡¡åˆ¤æ–­
    if is_imbalanced:
        print(f"  â†’ {f} ç» Imbalanced æ£€æµ‹ä¸ºä¸å¹³è¡¡å±æ€§")

# åˆ¤æ–­åˆ†ç±»æ¨¡å‹
if hasattr(model, "predict_proba"):
    # äºŒåˆ†ç±»æƒ…å†µä¸‹ï¼Œè®¡ç®— log loss
    proba = model.predict_proba([u])[0]
    true_label = y_test.iloc[outlier_idx]
    loss_val = log_loss([true_label], [proba], labels=[0, 1])

    print(f"  æ ·æœ¬çš„ log loss = {loss_val:.4f}")
    if loss_val > 0.5:
        print("  â†’ loss(u) = Trueï¼Œé«˜æŸå¤±æ ·æœ¬")
    else:
        print("  â†’ loss(u) = Falseï¼ŒæŸå¤±æ­£å¸¸")

print("\nğŸ§ª Mo è°“è¯æ£€æµ‹ï¼ˆä½¿ç”¨è¾…åŠ©æ¨¡å‹ï¼‰:")

# ä½¿ç”¨ Isolation Forest è¿›è¡Œè¾…åŠ©æ£€æµ‹
mo_model = IsolationForest(random_state=42)
mo_model.fit(X_train)

mo_pred = mo_model.predict([u])  # æ³¨æ„ï¼šè¾“å‡ºä¸º 1ï¼ˆæ­£å¸¸ï¼‰æˆ– -1ï¼ˆç¦»ç¾¤ï¼‰
if mo_pred[0] == -1:
    print("  â†’ Mo(u) = Trueï¼Œè¢«è¾…åŠ©æ¨¡å‹åˆ¤å®šä¸ºç¦»ç¾¤å€¼")
else:
    print("  â†’ Mo(u) = Falseï¼Œè¾…åŠ©æ¨¡å‹è®¤ä¸ºæ˜¯æ­£å¸¸æ ·æœ¬")

try:
    # å°† u è½¬ä¸º float32 Tensorï¼Œå¹¶æ·»åŠ  batch ç»´åº¦ï¼ˆå³ shape [1, d]ï¼‰
    mo_input = torch.tensor([u.values], dtype=torch.float32).to(out_clf.device)

    # === è°ƒç”¨å·²æœ‰çš„æ¨¡å‹æ¨ç†æ¥å£ ===
    mo_label = out_clf.predict(mo_input)[0]  # é€šå¸¸ 1 è¡¨ç¤ºæ­£å¸¸ï¼Œ-1 è¡¨ç¤ºç¦»ç¾¤

    if mo_pred[0] == -1:
        print("  â†’ Mo(u) = Trueï¼Œè¢«deepodæ¨¡å‹åˆ¤å®šä¸ºç¦»ç¾¤å€¼")
    else:
        print("  â†’ Mo(u) = Falseï¼Œdeepodæ¨¡å‹è®¤ä¸ºæ˜¯æ­£å¸¸æ ·æœ¬")

except Exception as e:
    print("  â†’ Mo æ£€æµ‹å¤±è´¥ï¼š", e)

