import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from scipy.stats import zscore
import shap
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('TkAgg')
from sklearn.model_selection import GridSearchCV
from scipy.stats import skew
from sklearn.metrics import log_loss, mean_squared_error
from sklearn.ensemble import IsolationForest
# å±è”½ sklearn çš„æ‰€æœ‰ warningï¼ˆåŒ…æ‹¬ç‰¹å¾åä¸åŒ¹é…çš„ï¼‰
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
import torch
from deepod.models import DeepSAD, RoSAS, PReNet
from deepod.models import REPEN, SLAD, ICL, NeuTraL
from torch.utils.data import TensorDataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

# === 6. åˆ†åˆ«åˆ†ææ¯ä¸ªç¦»ç¾¤æ ·æœ¬ ===
def is_outlier_zscore(val, col, threshold=1):
    z = (val - col.mean()) / col.std()
    return abs(z) > threshold, z

# ç¨€ç–æ€§è®¡ç®—å‡½æ•°
def compute_sparsity(outlier_indices, X_test, X_train, model, out_clf, y_test, threshold_loss=0.5):
    total_features = len(X_test.columns)  # æ€»ç‰¹å¾æ•°
    sparse_feature_count = 0  # æ»¡è¶³æ¡ä»¶çš„ç‰¹å¾æ•°

    for idx in outlier_indices:
        print("*" * 100)
        print(f"\n{'='*40}\næ ·æœ¬ idx = {idx}")
        u = X_test.iloc[idx]
        # print("ç›®æ ‡æ ·æœ¬ï¼š\n", u)

        # æå– SHAP å€¼
        shap_value = shap_values[idx]
        sample_values = shap_value.values
        feature_names = X.columns

        # 1. æ¨¡å‹é‡è¦ç‰¹å¾ï¼ˆMDetIï¼‰
        important_features_dict = {}
        # print("\næ¨¡å‹è®¤ä¸ºçš„é‡è¦ç‰¹å¾ï¼ˆMDetIï¼‰:")
        for name, value in zip(feature_names, sample_values):
            if abs(value) > 0.1:  # è‡ªå®šä¹‰é˜ˆå€¼
                important_features_dict[name] = value
                # print(f"  {name}: SHAP = {value:.3f}")

        # 2. ç¦»ç¾¤å€¼æ£€æµ‹ï¼ˆä»…é™é‡è¦ç‰¹å¾ï¼‰
        outlier_features = {}
        # print("\nOutlier æ£€æµ‹ï¼ˆä»…é™é‡è¦ç‰¹å¾ï¼‰:")
        for f in important_features_dict:
            col_vals = X_train[f]
            is_out, z = is_outlier_zscore(u[f], col_vals)
            if is_out:
                outlier_features[f] = z
                # print(f"  {f}: {u[f]} (z-score = {z:.2f}) â†’ æ˜¯ç¦»ç¾¤å€¼")
            else:
                continue
                # print(f"  {f}: {u[f]} (z-score = {z:.2f}) â†’ æ­£å¸¸")

        # 3. Imbalanced æ£€æµ‹ï¼ˆä»…å¯¹è¢«åˆ¤ä¸ºç¦»ç¾¤å€¼çš„ç‰¹å¾ï¼‰
        imbalanced_features = {}
        # print("\nImbalanced æ£€æµ‹ï¼ˆä»…å¯¹è¢«åˆ¤ä¸ºç¦»ç¾¤å€¼çš„ç‰¹å¾ï¼‰:")

        for f in outlier_features:  # åªå¯¹è¢«åˆ¤ä¸ºç¦»ç¾¤çš„ç‰¹å¾æ£€æµ‹åˆ†å¸ƒå˜åŒ–
            S = X_train[f]  # åŸå§‹è®­ç»ƒé›†ä¸­çš„æŸåˆ—
            S_plus_u = pd.concat([S, pd.Series(u[f])], ignore_index=True)  # åŠ å…¥å½“å‰æ ·æœ¬å€¼

            # === æ ‡å‡†å·®å˜åŒ–
            orig_std = S.std()
            new_std = S_plus_u.std()
            delta_std = abs(new_std - orig_std) / (orig_std + 1e-8)

            # === ååº¦å˜åŒ–ï¼ˆè¡¡é‡åˆ†å¸ƒå¯¹ç§°æ€§ï¼‰
            orig_skew = skew(S)
            new_skew = skew(S_plus_u)
            delta_skew = abs(new_skew - orig_skew)

            # === IQR æ£€æŸ¥ï¼ˆè¡¡é‡åˆ†å¸ƒå°¾éƒ¨å¼‚å¸¸å€¼ï¼‰
            Q1 = S.quantile(0.25)
            Q3 = S.quantile(0.75)
            IQR = Q3 - Q1

            # === åˆ¤æ–­æ˜¯å¦ä¸å¹³è¡¡
            is_imbalanced = False

            if delta_std > 0.02:
                # print(f"  {f}ï¼šæ ‡å‡†å·®å˜åŒ– Î” = {delta_std:.2%}")
                is_imbalanced = True
            elif delta_skew > 0.1:
                # print(f"  {f}ï¼šååº¦å˜åŒ– Î” = {delta_skew:.3f}")
                is_imbalanced = True
            elif u[f] < Q1 - 1.5 * IQR or u[f] > Q3 + 1.5 * IQR:
                # print(f"  {f}ï¼šå€¼ {u[f]} è¶…å‡º IQR èŒƒå›´ ({Q1:.2f}, {Q3:.2f})")
                is_imbalanced = True

            # === è¾“å‡ºä¸å¹³è¡¡åˆ¤æ–­
            if is_imbalanced:
                # print(f"  â†’ {f} ç» Imbalanced æ£€æµ‹ä¸ºä¸å¹³è¡¡å±æ€§")
                imbalanced_features[f] = True

        # 4. Loss(u) è°“è¯æ£€æµ‹
        high_loss_features = []
        if hasattr(model, "predict_proba"):
            proba = model.predict_proba([u])[0]
            true_label = y_test.iloc[idx]
            try:
                loss_val = log_loss([true_label], [proba], labels=[0, 1])
            except:
                loss_val = float("nan")

            # print(f"  ğŸ” log loss = {loss_val:.4f}")
            if loss_val > threshold_loss:
                # print("  â†’ loss(u) = Trueï¼Œé«˜æŸå¤±æ ·æœ¬")
                high_loss_features.append(u.name)

        # 5. ç»Ÿè®¡ç¬¦åˆæ¡ä»¶çš„ç‰¹å¾æ•°é‡
        # å¯¹ç¦»ç¾¤ã€ä¸å¹³è¡¡å’Œé«˜æŸå¤±çš„ç‰¹å¾è¿›è¡Œç»Ÿè®¡
        all_affected_features = set(outlier_features.keys()).union(set(imbalanced_features.keys())).union(set(high_loss_features))
        sparse_feature_count += len(all_affected_features)

    # 6. è®¡ç®—ç¨€ç–æ€§
    sparsity_score = sparse_feature_count / total_features
    # print(f"\nç¨€ç–æ€§è¯„åˆ†ï¼š{sparsity_score:.4f}")
    return sparsity_score


def compute_fidelity(outlier_indices, X_test, X_train, model, out_clf, y_test, threshold_loss=0.5):
    """
    è®¡ç®—å¿ å®åº¦ï¼ˆFidelityï¼‰è¯„åˆ†ã€‚

    outlier_indices: ç¦»ç¾¤å€¼æ ·æœ¬çš„ç´¢å¼•
    X_test: æµ‹è¯•é›†ç‰¹å¾
    X_train: è®­ç»ƒé›†ç‰¹å¾
    model: ä¸»è¦çš„åˆ†ç±»æ¨¡å‹
    out_clf: è¾…åŠ©æ¨¡å‹ï¼ˆç”¨äºç¦»ç¾¤å€¼æ£€æµ‹ï¼‰
    y_test: æµ‹è¯•é›†æ ‡ç­¾
    threshold_loss: é«˜æŸå¤±å€¼çš„é˜ˆå€¼ï¼Œé»˜è®¤0.5
    """

    total_outliers = len(outlier_indices)
    satisfied_count = 0  # æ»¡è¶³æ¡ä»¶çš„ç¦»ç¾¤æ ·æœ¬æ•°

    for idx in outlier_indices:
        # print("*" * 100)
        # print(f"\n{'=' * 40}\næ ·æœ¬ idx = {idx}")
        u = X_test.iloc[idx]
        # print("ç›®æ ‡æ ·æœ¬ï¼š\n", u)

        # æå– SHAP å€¼
        shap_value = shap_values[idx]
        sample_values = shap_value.values
        feature_names = X.columns

        # æ¨¡å‹é‡è¦ç‰¹å¾ï¼ˆMDetIï¼‰
        important_features_dict = {}
        for name, value in zip(feature_names, sample_values):
            if abs(value) > 0.1:  # è‡ªå®šä¹‰é˜ˆå€¼
                important_features_dict[name] = value

        # ç¦»ç¾¤å€¼æ£€æµ‹ï¼ˆä»…é™é‡è¦ç‰¹å¾ï¼‰
        outlier_features = {}
        is_outlier_feature = False
        for f in important_features_dict:
            col_vals = X_train[f]
            is_out, z = is_outlier_zscore(u[f], col_vals)
            if is_out:
                outlier_features[f] = z
                is_outlier_feature = True

        # Imbalanced æ£€æµ‹ï¼ˆä»…å¯¹è¢«åˆ¤ä¸ºç¦»ç¾¤å€¼çš„ç‰¹å¾ï¼‰
        is_imbalanced = False
        for f in outlier_features:  # åªå¯¹è¢«åˆ¤ä¸ºç¦»ç¾¤çš„ç‰¹å¾æ£€æµ‹åˆ†å¸ƒå˜åŒ–
            S = X_train[f]  # åŸå§‹è®­ç»ƒé›†ä¸­çš„æŸåˆ—
            S_plus_u = pd.concat([S, pd.Series(u[f])], ignore_index=True)  # åŠ å…¥å½“å‰æ ·æœ¬å€¼

            # æ ‡å‡†å·®å˜åŒ–
            orig_std = S.std()
            new_std = S_plus_u.std()
            delta_std = abs(new_std - orig_std) / (orig_std + 1e-8)

            # ååº¦å˜åŒ–
            orig_skew = skew(S)
            new_skew = skew(S_plus_u)
            delta_skew = abs(new_skew - orig_skew)

            # IQR æ£€æŸ¥
            Q1 = S.quantile(0.25)
            Q3 = S.quantile(0.75)
            IQR = Q3 - Q1

            if delta_std > 0.02 or delta_skew > 0.1 or u[f] < Q1 - 1.5 * IQR or u[f] > Q3 + 1.5 * IQR:
                is_imbalanced = True

        # log_loss æ£€æŸ¥
        proba = model.predict_proba([u])[0]
        true_label = y_test.iloc[idx]
        try:
            loss_val = log_loss([true_label], [proba], labels=[0, 1])
        except:
            loss_val = float("nan")

        # æ»¡è¶³ loss é˜ˆå€¼
        is_high_loss = loss_val > threshold_loss

        # ç»Ÿè®¡æ»¡è¶³æ¡ä»¶çš„æ ·æœ¬
        if is_outlier_feature or is_imbalanced or is_high_loss:
            satisfied_count += 1

        # è¾“å‡ºæ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
        # print(f"  ğŸ” log loss = {loss_val:.4f}")
        # print(f"  â†’ ç¦»ç¾¤å€¼ç‰¹å¾: {is_outlier_feature}")
        # print(f"  â†’ ä¸å¹³è¡¡ç‰¹å¾: {is_imbalanced}")
        # print(f"  â†’ é«˜æŸå¤±æ ·æœ¬: {is_high_loss}")

    # è®¡ç®—å¿ å®åº¦
    fidelity_score = satisfied_count / total_outliers if total_outliers > 0 else 0.0
    return fidelity_score

# subsection é€‰ç”¨çš„å¤§è§„æ¨¡æ•°æ®é›†
file_path = "../../../datasets/multi_class/adult.csv"
data = pd.read_csv(file_path)

# subsection é€‰ç”¨çš„å¤§è§„æ¨¡æ•°æ®é›†
# file_path = "../../../datasets/multi_class/flights.csv"
# data = pd.read_csv(file_path).dropna(subset=['WEATHER_DELAY'])

# å¦‚æœæ•°æ®é‡è¶…è¿‡20000è¡Œï¼Œå°±éšæœºé‡‡æ ·åˆ°20000è¡Œ
if len(data) > 10000:
    data = data.sample(n=10000, random_state=42)

enc = LabelEncoder()
label_name = data.columns[-1]

# åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
data[label_name] = enc.fit_transform(data[label_name])

# æ£€æµ‹éæ•°å€¼åˆ—
non_numeric_columns = data.select_dtypes(exclude=[np.number]).columns

# ä¸ºæ¯ä¸ªéæ•°å€¼åˆ—åˆ›å»ºä¸€ä¸ª LabelEncoder å®ä¾‹
encoders = {}
for column in non_numeric_columns:
    encoder = LabelEncoder()
    data[column] = encoder.fit_transform(data[column])
    encoders[column] = encoder  # ä¿å­˜æ¯ä¸ªåˆ—çš„ç¼–ç å™¨ï¼Œä»¥ä¾¿å°†æ¥å¯èƒ½éœ€è¦è§£ç 

# section æ•°æ®ç‰¹å¾ç¼©æ”¾å’Œæ•°æ®åŠ å™ª

X = data.iloc[:, :-1]
y = data.iloc[:, -1]
# æ•°æ®æ ‡å‡†åŒ–ï¼ˆå¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼‰
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X = pd.DataFrame(X_scaled, columns=X.columns)

# === 2. æ‹†åˆ†è®­ç»ƒ / æµ‹è¯• / éªŒè¯ ===
original_indices = np.arange(len(X))
# æŒ‰ç…§ 70% è®­ç»ƒé›†ï¼Œ20% éªŒè¯é›†ï¼Œ10% æµ‹è¯•é›†çš„æ¯”ä¾‹éšæœºåˆ’åˆ†æ•°æ®é›†
X_train, X_temp, y_train, y_temp, train_indices, temp_indices = train_test_split(X, y, original_indices, test_size=0.3, random_state=1)
# ä»ä¸´æ—¶æ•°æ®é›†ï¼ˆ30%ï¼‰ä¸­åˆ’åˆ†å‡º 10% æµ‹è¯•é›†å’Œ 20% éªŒè¯é›†
X_val, X_test, y_val, y_test, val_indices, test_indices = train_test_split(X_temp, y_temp, temp_indices, test_size=0.33, random_state=1)

# === 3. è®­ç»ƒ XGBoost æ¨¡å‹ ===
# å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
param_grid = {
    'n_estimators': [50, 100, 150],      # æ ‘çš„æ•°é‡
    'max_depth': [3, 6, 9],              # æ ‘çš„æœ€å¤§æ·±åº¦
    'learning_rate': [0.01, 0.1, 0.2],   # å­¦ä¹ ç‡
    'subsample': [0.8, 1.0],             # å­æ ·æœ¬æ¯”ä¾‹
    'colsample_bytree': [0.8, 1.0],      # æ¯æ£µæ ‘çš„ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
    'scale_pos_weight': [1, 2],          # ç±»åˆ«ä¸å¹³è¡¡æ—¶è°ƒèŠ‚
    'class_weight': [None]               # å…¼å®¹åŸæ ¼å¼ï¼ŒXGBoost å®é™…ä¸æ”¯æŒ class_weightï¼ˆå ä½ï¼‰
}

# åˆå§‹åŒ– XGBoost æ¨¡å‹ï¼ˆä¸ç”¨æŒ‡å®š n_estimators ä¹‹ç±»çš„ï¼Œè¿™äº›ä¼šç”± GridSearch å†³å®šï¼‰
model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    booster='gbtree',
    random_state=42,
    verbosity=0
)

# åˆå§‹åŒ– GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           scoring='accuracy',
                           cv=2,
                           verbose=2,
                           n_jobs=-1)

# æ‹Ÿåˆæ¨¡å‹
grid_search.fit(X_val, y_val)

# è¾“å‡ºæœ€ä½³è¶…å‚æ•°å’Œäº¤å‰éªŒè¯å¾—åˆ†
print("æœ€ä½³è¶…å‚æ•°ç»„åˆï¼š", grid_search.best_params_)
print("æœ€ä½³äº¤å‰éªŒè¯å‡†ç¡®åº¦ï¼š", grid_search.best_score_)

# è·å–æœ€ä½³æ¨¡å‹
model = grid_search.best_estimator_
model.fit(X_train, y_train)

# === 4. é¢„æµ‹ & é€‰å–å‰10ä¸ªè¢«è¯†åˆ«ä¸ºç¦»ç¾¤å€¼çš„æ ·æœ¬ ===
y_pred = model.predict(X_test)
outlier_indices = np.where(y_pred == 1)[0][:10]  # å‰10ä¸ªé¢„æµ‹ä¸ºç¦»ç¾¤å€¼çš„æ ·æœ¬

# === 5. æ¨¡å‹ SHAP è§£é‡Šå™¨åˆå§‹åŒ– ===
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)

# è¾…åŠ©æ£€æµ‹æ¨¡å‹ï¼ˆå¦‚ Isolation Forestï¼‰
mo_model = IsolationForest(random_state=42)
mo_model.fit(X_train)

# choice NeuTraLå¼‚å¸¸æ£€æµ‹å™¨
epochs = 1
device = 'cuda' if torch.cuda.is_available() else 'cpu'
n_trans = 64
random_state = 42
out_clf = NeuTraL(epochs=1, device=device)
# è½¬æ¢ä¸º float32 çš„ numpy æ•°ç»„
X_tensor = torch.tensor(X_train.values, dtype=torch.float32)
out_clf.fit(X_tensor)

for idx in outlier_indices:
    print("*"*100)
    print(f"\n{'='*40}\næ ·æœ¬ idx = {idx}")
    u = X_test.iloc[idx]
    print("ç›®æ ‡æ ·æœ¬ï¼š\n", u)

    # æå– SHAP å€¼
    shap_value = shap_values[idx]
    sample_values = shap_value.values
    feature_names = X.columns

    # æ¨¡å‹é‡è¦ç‰¹å¾ï¼ˆMDetIï¼‰
    important_features_dict = {}
    print("\næ¨¡å‹è®¤ä¸ºçš„é‡è¦ç‰¹å¾ï¼ˆMDetIï¼‰:")
    for name, value in zip(feature_names, sample_values):
        if abs(value) > 0.1:  # è‡ªå®šä¹‰é˜ˆå€¼
            important_features_dict[name] = value
            print(f"  {name}: SHAP = {value:.3f}")

    # ç¦»ç¾¤å€¼æ£€æµ‹ï¼ˆä»…é™é‡è¦ç‰¹å¾ï¼‰
    outlier_features = {}
    print("\nOutlier æ£€æµ‹ï¼ˆä»…é™é‡è¦ç‰¹å¾ï¼‰:")
    for f in important_features_dict:
        col_vals = X_train[f]
        is_out, z = is_outlier_zscore(u[f], col_vals)
        if is_out:
            outlier_features[f] = z
            print(f"  {f}: {u[f]} (z-score = {z:.2f}) â†’ æ˜¯ç¦»ç¾¤å€¼")
        else:
            print(f"  {f}: {u[f]} (z-score = {z:.2f}) â†’ æ­£å¸¸")

    print("\nImbalanced æ£€æµ‹ï¼ˆä»…å¯¹è¢«åˆ¤ä¸ºç¦»ç¾¤å€¼çš„ç‰¹å¾ï¼‰:")

    for f in outlier_features:  # åªå¯¹è¢«åˆ¤ä¸ºç¦»ç¾¤çš„ç‰¹å¾æ£€æµ‹åˆ†å¸ƒå˜åŒ–
        S = X_train[f]  # åŸå§‹è®­ç»ƒé›†ä¸­çš„æŸåˆ—
        S_plus_u = pd.concat([S, pd.Series(u[f])], ignore_index=True)  # åŠ å…¥å½“å‰æ ·æœ¬å€¼

        # === æ ‡å‡†å·®å˜åŒ–
        orig_std = S.std()
        new_std = S_plus_u.std()
        delta_std = abs(new_std - orig_std) / (orig_std + 1e-8)

        # === ååº¦å˜åŒ–ï¼ˆè¡¡é‡åˆ†å¸ƒå¯¹ç§°æ€§ï¼‰
        orig_skew = skew(S)
        new_skew = skew(S_plus_u)
        delta_skew = abs(new_skew - orig_skew)

        # === IQR æ£€æŸ¥ï¼ˆè¡¡é‡åˆ†å¸ƒå°¾éƒ¨å¼‚å¸¸å€¼ï¼‰
        Q1 = S.quantile(0.25)
        Q3 = S.quantile(0.75)
        IQR = Q3 - Q1

        # === åˆ¤æ–­æ˜¯å¦ä¸å¹³è¡¡
        is_imbalanced = False

        if delta_std > 0.02:
            print(f"  {f}ï¼šæ ‡å‡†å·®å˜åŒ– Î” = {delta_std:.2%}")
            is_imbalanced = True
        elif delta_skew > 0.1:
            print(f"  {f}ï¼šååº¦å˜åŒ– Î” = {delta_skew:.3f}")
            is_imbalanced = True
        elif u[f] < Q1 - 1.5 * IQR or u[f] > Q3 + 1.5 * IQR:
            print(f"  {f}ï¼šå€¼ {u[f]} è¶…å‡º IQR èŒƒå›´ ({Q1:.2f}, {Q3:.2f})")
            is_imbalanced = True

        # === è¾“å‡ºä¸å¹³è¡¡åˆ¤æ–­
        if is_imbalanced:
            print(f"  â†’ {f} ç» Imbalanced æ£€æµ‹ä¸ºä¸å¹³è¡¡å±æ€§")

    u = X_test.iloc[idx]
    # === Loss(u) è°“è¯ ===
    if hasattr(model, "predict_proba"):
        proba = model.predict_proba([u])[0]
        true_label = y_test.iloc[idx]
        try:
            loss_val = log_loss([true_label], [proba], labels=[0, 1])
        except:
            loss_val = float("nan")

        print(f"  ğŸ” log loss = {loss_val:.4f}")
        if loss_val > 0.5:
            print("  â†’ loss(u) = Trueï¼Œé«˜æŸå¤±æ ·æœ¬")
        else:
            print("  â†’ loss(u) = Falseï¼ŒæŸå¤±æ­£å¸¸")

    print("\nğŸ§ª Mo è°“è¯æ£€æµ‹ï¼ˆä½¿ç”¨è¾…åŠ©æ¨¡å‹ out_clfï¼ŒTensor è¾“å…¥ï¼‰:")

    try:
        # å°† u è½¬ä¸º float32 Tensorï¼Œå¹¶æ·»åŠ  batch ç»´åº¦ï¼ˆå³ shape [1, d]ï¼‰
        mo_input = torch.tensor([u.values], dtype=torch.float32).to(out_clf.device)

        # === è°ƒç”¨å·²æœ‰çš„æ¨¡å‹æ¨ç†æ¥å£ ===
        mo_label = out_clf.predict(mo_input)[0]  # é€šå¸¸ 1 è¡¨ç¤ºæ­£å¸¸ï¼Œ-1 è¡¨ç¤ºç¦»ç¾¤

        # === å¯é€‰è¾“å‡ºï¼šconfidence åˆ†æ•°ï¼ˆå¦‚æœæ¨¡å‹æœ‰å¯¹åº”æ–¹æ³•ï¼‰
        if hasattr(out_clf, 'decision_function'):
            mo_score = out_clf.decision_function(mo_input)[0]
            print(f"  â†’ ç¦»ç¾¤åˆ¤æ–­: {mo_label}ï¼ˆscore = {mo_score:.4f}ï¼‰")
        elif hasattr(out_clf, 'predict_proba'):
            proba = out_clf.predict_proba(mo_input)[0]
            print(f"  â†’ ç¦»ç¾¤åˆ¤æ–­: {mo_label}ï¼ˆproba = {proba}ï¼‰")
        else:
            print(f"  â†’ ç¦»ç¾¤åˆ¤æ–­: {mo_label}")

        # === åˆ¤å®š Mo(u)
        if mo_label == -1:
            print("  â†’ Mo(u) = Trueï¼Œè¢« out_clf åˆ¤å®šä¸ºç¦»ç¾¤å€¼")
        else:
            print("  â†’ Mo(u) = Falseï¼Œout_clf åˆ¤å®šä¸ºæ­£å¸¸")

    except Exception as e:
        print("  â†’ Mo æ£€æµ‹å¤±è´¥ï¼š", e)

    # === Mo(u) è°“è¯ ===
    print("  ğŸ§ª Mo è°“è¯æ£€æµ‹ï¼ˆä½¿ç”¨è¾…åŠ©æ¨¡å‹ï¼‰:")
    mo_pred = mo_model.predict([u])
    if mo_pred[0] == -1:
        print("  â†’ Mo(u) = Trueï¼Œè¢«è¾…åŠ©æ¨¡å‹åˆ¤å®šä¸ºç¦»ç¾¤å€¼")
    else:
        print("  â†’ Mo(u) = Falseï¼Œè¾…åŠ©æ¨¡å‹è®¤ä¸ºæ˜¯æ­£å¸¸æ ·æœ¬")

"""
è®¡ç®—ç¨€ç–æ€§ï¼š
ç¨€ç–æ€§ï¼ˆSparsityï¼‰ è¡¡é‡äº†LIMEè§£é‡Šä¸­ä½¿ç”¨çš„ç‰¹å¾æ•°é‡ç›¸å¯¹äºæ¨¡å‹ç‰¹å¾æ€»æ•°çš„æ¯”ä¾‹ã€‚é€šè¿‡è®¡ç®—è¿™ä¸€æ¯”ä¾‹ï¼Œæ‚¨å¯ä»¥é‡åŒ–è§£é‡Šçš„ç®€æ´ç¨‹åº¦ï¼Œå¹¶è¿›ä¸€æ­¥ç ”ç©¶ç”¨æˆ·å¯¹äºä¸åŒç¨€ç–æ€§æ°´å¹³çš„æ¥å—åº¦ã€‚
 """
# éå† outlier_indices ä¸­çš„æ¯ä¸ªæ ·æœ¬ï¼Œè®¡ç®—ç¨€ç–æ€§
sparsity_scores = []
for idx in outlier_indices:
    idx_array = np.array([idx])  # å°† idx è½¬æ¢ä¸º numpy æ•°ç»„
    sparse_score = compute_sparsity(idx_array, X_test, X_train, model, out_clf, y_test, threshold_loss=0.5)
    sparsity_scores.append(sparse_score)

# è¾“å‡ºæ‰€æœ‰æ ·æœ¬çš„ç¨€ç–æ€§åˆ†æ•°
print("è¿™äº›æ ·æœ¬çš„ç¨€ç–æ€§åˆ†æ•°ä¸ºï¼š")
for idx, score in zip(outlier_indices, sparsity_scores):
    print(f"æ ·æœ¬ idx = {idx}, ç¨€ç–æ€§åˆ†æ•° = {score:.4f}")

# è®¡ç®—å‡å€¼
mean_sparsity = np.mean(sparsity_scores)

# æ‰“å°æ±‡æŠ¥ç»“æœ
print(f"å¹³å‡ç¨€ç–æ€§è¯„åˆ†ä¸ºï¼š{mean_sparsity:.4f}")

"""
è®¡ç®—å¿ å®åº¦ï¼š
 """
# ä½¿ç”¨è¯¥å‡½æ•°è®¡ç®—å¿ å®åº¦è¯„åˆ†
print("="*30)
fidelity_score = compute_fidelity(outlier_indices, X_test, X_train, model, out_clf, y_test, threshold_loss=0.5)
print("è¿™äº›ç¦»ç¾¤æ ·æœ¬çš„å¿ å®åº¦è¯„åˆ†ä¸ºï¼š", fidelity_score)

"""
è®¡ç®—å¿ å®åº¦ï¼š
 """
# # ä½¿ç”¨è¯¥å‡½æ•°è®¡ç®—å¿ å®åº¦è¯„åˆ†
# print("="*30)
# fidelity_scores = []
# for idx in outlier_indices:
#     idx_array = np.array([idx])  # å°† idx è½¬æ¢ä¸º numpy æ•°ç»„
#     fidelity_score = compute_fidelity(idx_array, X_test, X_train, model, out_clf, y_test, threshold_loss=0.5)
#     fidelity_scores.append(fidelity_score)
# # è¾“å‡ºæ‰€æœ‰æ ·æœ¬çš„ç¨€ç–æ€§åˆ†æ•°
# print("è¿™äº›æ ·æœ¬çš„å¿ å®åº¦åˆ†æ•°ä¸ºï¼š")
# for idx, score in zip(outlier_indices, fidelity_scores):
#     print(f"æ ·æœ¬ idx = {idx}, ç¨€ç–æ€§åˆ†æ•° = {score:.4f}")