"""
å‘ç°æ£€æµ‹ugly outliers çš„RoDsè§„åˆ™
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
import torch

from deepod.models.tabular import GOAD
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import KNNImputer
from lime.lime_tabular import LimeTabularExplainer
from deepod.models.tabular import DeepSVDD
from deepod.models.tabular import RCA
from deepod.models import REPEN, SLAD, ICL, NeuTraL
import re
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import precision_recall_curve, auc
from sklearn.metrics import average_precision_score
from sklearn.preprocessing import OneHotEncoder
from scipy.special import softmax
from sklearn.preprocessing import MinMaxScaler
import optuna
import os

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

# section æ•°æ®é¢„å¤„ç†
file_path = "../datasets/multi_class/drybean.xlsx"
data = pd.read_excel(file_path)
enc = LabelEncoder()
label_name = data.columns[-1]

# åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
data[label_name] = enc.fit_transform(data[label_name])

# æ£€æµ‹éæ•°å€¼åˆ—
non_numeric_columns = data.select_dtypes(exclude=[np.number]).columns

# ä¸ºæ¯ä¸ªéæ•°å€¼åˆ—åˆ›å»ºä¸€ä¸ª LabelEncoder å®ä¾‹
encoders = {}
for column in non_numeric_columns:
    encoder = LabelEncoder()
    data[column] = encoder.fit_transform(data[column])
    encoders[column] = encoder  # ä¿å­˜æ¯ä¸ªåˆ—çš„ç¼–ç å™¨ï¼Œä»¥ä¾¿å°†æ¥å¯èƒ½éœ€è¦è§£ç 

X = data.values[:, :-1]
y = data.values[:, -1]

# æ‰¾åˆ°åˆ†ç±»ç‰¹å¾çš„åˆ—å
categorical_columns = data.select_dtypes(exclude=['float']).columns[:-1]
# è·å–åˆ†ç±»ç‰¹å¾å¯¹åº”çš„ç´¢å¼•
categorical_features = [data.columns.get_loc(col) for col in categorical_columns]

# ç»Ÿè®¡ä¸åŒå€¼åŠå…¶æ•°é‡
unique_values, counts = np.unique(y, return_counts=True)

# è¾“å‡ºç»“æœ
for value, count in zip(unique_values, counts):
    print(f"æ ‡ç­¾: {value}, æ•°é‡: {count}")

# æ‰¾åˆ°æœ€å°æ ‡ç­¾çš„æ•°é‡
min_count = counts.min()
total_count = counts.sum()

# è®¡ç®—æ¯”ä¾‹
proportion = min_count / total_count
print(f"è¾ƒå°‘æ ‡ç­¾å æ®çš„æ¯”ä¾‹: {proportion:.4f}")
min_count_index = np.argmin(counts)  # æ‰¾åˆ°æœ€å°æ•°é‡çš„ç´¢å¼•
min_label = unique_values[min_count_index]  # å¯¹åº”çš„æ ‡ç­¾å€¼

# section æ•°æ®ç‰¹å¾ç¼©æ”¾

# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)

# è®°å½•åŸå§‹ç´¢å¼•
original_indices = np.arange(len(X))

# ä¿®æ”¹ï¼šé‡æ–°åˆ’åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
# é¦–å…ˆåˆ’åˆ†å‡ºè®­ç»ƒé›†å’Œä¸´æ—¶é›†ï¼ˆéªŒè¯é›† + æµ‹è¯•é›†ï¼‰
X_train, X_temp, y_train, y_temp, train_indices, temp_indices = train_test_split(
    X, y, original_indices, test_size=0.3, random_state=42
)

# ä»ä¸´æ—¶é›†å†åˆ’åˆ†éªŒè¯é›†å’Œæµ‹è¯•é›†
X_val, X_test, y_val, y_test, val_indices, test_indices = train_test_split(
    X_temp, y_temp, temp_indices, test_size=1/3, random_state=42
)

# åŠ å…¥éšæœºå™ªå£°çš„æ¯”ä¾‹
noise_level = 0.2
# è®¡ç®—å™ªå£°æ•°é‡
n_samples = X.shape[0]
n_noise = int(noise_level * n_samples)
# éšæœºé€‰æ‹©è¦æ·»åŠ å™ªå£°çš„æ ·æœ¬
noise_indices = np.random.choice(n_samples, n_noise, replace=False)
# æ·»åŠ é«˜æ–¯å™ªå£°åˆ°ç‰¹å¾
X_copy = np.copy(X)
X_copy[noise_indices] += np.random.normal(0, 1, (n_noise, X.shape[1]))
# ä»å«å™ªæ•°æ®ä¸­ç”Ÿæˆè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
X_train_copy = X_copy[train_indices]
X_val_copy = X_copy[val_indices]
X_test_copy = X_copy[test_indices]

# æ„å»º DataFrame å¹¶åˆå¹¶ y å€¼
feature_names = data.columns.values.tolist()  # Assuming `data` is a DataFrame that contains feature names

# å„è‡ªçš„ DataFrame
train_data_copy = pd.DataFrame(np.hstack((X_train_copy, y_train.reshape(-1, 1))), columns=feature_names)
val_data_copy = pd.DataFrame(np.hstack((X_val_copy, y_val.reshape(-1, 1))), columns=feature_names)
test_data_copy = pd.DataFrame(np.hstack((X_test_copy, y_test.reshape(-1, 1))), columns=feature_names)

# è¾“å‡ºåˆ’åˆ†ç»“æœ
print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]} æ ·æœ¬")
print(f"éªŒè¯é›†å¤§å°: {X_val.shape[0]} æ ·æœ¬")
print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]} æ ·æœ¬")

# è®­ç»ƒé›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
train_noise = np.intersect1d(train_indices, noise_indices)
# éªŒè¯é›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
val_noise = np.intersect1d(val_indices, noise_indices)
# æµ‹è¯•é›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
test_noise = np.intersect1d(test_indices, noise_indices)

print(f"è®­ç»ƒé›†å™ªå£°æ ·æœ¬æ•°é‡: {len(train_noise)}")
print(f"éªŒè¯é›†å™ªå£°æ ·æœ¬æ•°é‡: {len(val_noise)}")
print(f"æµ‹è¯•é›†å™ªå£°æ ·æœ¬æ•°é‡: {len(test_noise)}")

# æœ€åè¾“å‡º DataFrame ç±»å‹
print("è®­ç»ƒé›† DataFrame:")
print(train_data_copy.head())

print("éªŒè¯é›† DataFrame:")
print(val_data_copy.head())

print("æµ‹è¯•é›† DataFrame:")
print(test_data_copy.head())


# SECTION SVMæ¨¡å‹çš„å®ç°

# subsection åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„SVMæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

print("*" * 100)
svm_model = svm.SVC(kernel='rbf', C=2.87, probability=True, class_weight='balanced')

# åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒæ¨¡å‹
svm_model.fit(X_train, y_train)

# åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼ˆç”¨æ¥é€‰æ‹©è¶…å‚æ•°ç­‰ï¼Œä¸ç›´æ¥å½±å“è®­ç»ƒï¼‰
val_label_pred = svm_model.predict(X_val)
wrong_classified_val_indices = np.where(y_val != val_label_pred)[0]
print("éªŒè¯é›†ä¸Šè¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»éªŒè¯æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_val_indices)/len(y_val))

# åœ¨è®­ç»ƒé›†ä¸Šé¢„æµ‹å¹¶è¯„ä¼°æ¨¡å‹
train_label_pred = svm_model.predict(X_train)
wrong_classified_train_indices = np.where(y_train != train_label_pred)[0]
print("è®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))

# åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹å¹¶è¯„ä¼°æ¨¡å‹ï¼ˆç”¨äºæœ€ç»ˆè¯„ä¼°ï¼‰
test_label_pred = svm_model.predict(X_test)
wrong_classified_test_indices = np.where(y_test != test_label_pred)[0]
print("æµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))

# æ•´ä½“æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
      (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))

# subsection åŠ å™ªæ•°æ®é›†ä¸Šè®­ç»ƒçš„SVMæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

print("*" * 100)
svm_model_noise = svm.SVC(kernel='rbf', C=2.87, probability=True, class_weight='balanced')

# åœ¨åŠ å™ªè®­ç»ƒé›†ä¸Šè®­ç»ƒæ¨¡å‹
svm_model_noise.fit(X_train_copy, y_train)

# åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼ˆç”¨æ¥é€‰æ‹©è¶…å‚æ•°ç­‰ï¼Œä¸ç›´æ¥å½±å“è®­ç»ƒï¼‰
val_label_pred_noise = svm_model_noise.predict(X_val)
wrong_classified_val_indices_noise = np.where(y_val != val_label_pred_noise)[0]
print("éªŒè¯é›†ä¸ŠåŠ å™ªæ¨¡å‹è¢«SVMé”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ¯”ï¼š", len(wrong_classified_val_indices_noise)/len(y_val))

# åœ¨åŠ å™ªè®­ç»ƒé›†ä¸Šé¢„æµ‹å¹¶è¯„ä¼°æ¨¡å‹
train_label_pred_noise = svm_model_noise.predict(X_train_copy)
wrong_classified_train_indices_noise = np.where(y_train != train_label_pred_noise)[0]
print("åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»åŠ å™ªè®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices_noise)/len(y_train))

# åœ¨åŠ å™ªæµ‹è¯•é›†ä¸Šé¢„æµ‹å¹¶è¯„ä¼°æ¨¡å‹ï¼ˆç”¨äºæœ€ç»ˆè¯„ä¼°ï¼‰
test_label_pred_noise = svm_model_noise.predict(X_test_copy)
wrong_classified_test_indices_noise = np.where(y_test != test_label_pred_noise)[0]
print("åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices_noise)/len(y_test))

# æ•´ä½“åŠ å™ªæ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("å®Œæ•´åŠ å™ªæ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
      (len(wrong_classified_train_indices_noise) + len(wrong_classified_test_indices_noise))/(len(y_train) + len(y_test)))

# SECTION æ£€æµ‹æœ‰å½±å“åŠ›çš„ç‰¹å¾MDetO(ğ‘¡,ğ´,D)çš„å®ç°
# choice LIME(Local Interpretable Model-Agnostic Explanation)(æ•ˆæœå¥½)
def MDetI(clf, X_train_copy, X_val_copy, feature_names):
    # æ‰¾åˆ°åˆ†ç±»ç‰¹å¾çš„åˆ—å
    categorical_columns = [col for col in feature_names if isinstance(X_train_copy[0], str)]  # Assuming categorical columns are strings
    # è·å–åˆ†ç±»ç‰¹å¾å¯¹åº”çš„ç´¢å¼•
    categorical_features = [feature_names.index(col) for col in categorical_columns]

    # å¯¹æ¯ä¸ªåˆ†ç±»ç‰¹å¾è¿›è¡Œç¼–ç 
    np.random.seed(1)
    categorical_names = {}
    for feature in categorical_features:
        le = LabelEncoder()
        le.fit(X_train_copy[:, feature])  # Assuming that the columns are index-based, like in NumPy arrays
        X_train_copy[:, feature] = le.transform(X_train_copy[:, feature])
        X_val_copy[:, feature] = le.transform(X_val_copy[:, feature])  # Ensure val data is also transformed
        categorical_names[feature] = le.classes_

    # åˆ›å»º LimeTabularExplainer å®ä¾‹
    explainer = LimeTabularExplainer(X_train_copy, feature_names=feature_names, class_names=feature_names,
                                     categorical_features=categorical_features,
                                     categorical_names=categorical_names, kernel_width=3)

    # å®šä¹‰é¢„æµ‹å‡½æ•°ï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡
    predict_fn = lambda x: clf.predict_proba(x)

    # åœ¨è®­ç»ƒé›†ä¸Šè§£é‡Š
    exp_train = explainer.explain_instance(X_train_copy[0], predict_fn, num_features=len(feature_names) // 2)
    top_features_train = exp_train.as_list()
    top_feature_names_train = [re.search(r'([a-zA-Z_]\w*)', feature[0]).group(0).strip() for feature in top_features_train]
    top_k_indices_train = [feature_names.index(name) for name in top_feature_names_train]

    # åœ¨éªŒè¯é›†ä¸Šè§£é‡Š
    exp_val = explainer.explain_instance(X_val_copy[0], predict_fn, num_features=len(feature_names) // 2)
    top_features_val = exp_val.as_list()
    top_feature_names_val = [re.search(r'([a-zA-Z_]\w*)', feature[0]).group(0).strip() for feature in top_features_val]
    top_k_indices_val = [feature_names.index(name) for name in top_feature_names_val]

    return top_k_indices_train, top_k_indices_val

# SECTION MDetO(ğ‘¡,ğ´,D) é’ˆå¯¹å…ƒç»„å¼‚å¸¸çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨GOADçš„å®ç°
def MDetO(outlier_detector, X_train_copy, X_test_copy, X_copy):
    epochs = 5
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    n_trans = 64
    random_state = 42

    if outlier_detector == "GOAD":
        out_clf_noise = GOAD(epochs=epochs, device=device, n_trans=n_trans)
        out_clf_noise.fit(X_train_copy, y=None)
    elif outlier_detector == "RCA":
        out_clf_noise = RCA(epochs=epochs, device=device, act='LeakyReLU')
        out_clf_noise.fit(X_train_copy, y=None)
    else:
        out_clf_noise = SLAD(epochs=epochs, device=device)
        out_clf_noise.fit(X_train_copy, y=None)

    # ä»åŠ å™ªè®­ç»ƒé›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•
    train_pred_labels_noise, train_confidence_noise = out_clf_noise.predict(X_train_copy, return_confidence=True)
    print("åŠ å™ªè®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf_noise.threshold_)
    train_outliers_index_noise = []
    print("åŠ å™ªè®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train_copy))
    for i in range(len(X_train_copy)):
        if train_pred_labels_noise[i] == 1:
            train_outliers_index_noise.append(i)
    print("åŠ å™ªè®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index_noise)

    # ä»åŠ å™ªæµ‹è¯•é›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•
    test_pred_labels_noise, test_confidence_noise = out_clf_noise.predict(X_test_copy, return_confidence=True)
    print("åŠ å™ªæµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf_noise.threshold_)
    test_outliers_index_noise = []
    print("åŠ å™ªæµ‹è¯•é›†æ ·æœ¬æ•°ï¼š", len(X_test_copy))
    for i in range(len(X_test_copy)):
        if test_pred_labels_noise[i] == 1:
            test_outliers_index_noise.append(i)
    # è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
    print("åŠ å™ªæµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", test_outliers_index_noise)

    # ä»æ•´ä½“çš„åŠ å™ªæ•°æ®é›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•
    pred_labels_noise, onfidence_noise = out_clf_noise.predict(X_copy, return_confidence=True)
    print("åŠ å™ªæ•°æ®é›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf_noise.threshold_)
    outliers_index_noise = []
    print("åŠ å™ªæ•°æ®é›†æ ·æœ¬æ•°ï¼š", len(X_copy))
    for i in range(len(X_copy)):
        if pred_labels_noise[i] == 1:
            outliers_index_noise.append(i)
    # è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
    print("åŠ å™ªæ•°æ®é›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", outliers_index_noise)

    return train_outliers_index_noise, test_outliers_index_noise, outliers_index_noise

# section outlier(ğ·,ğ‘…,ğ‘¡,ğ´,ğœƒ)çš„å®ç°
def outlier(data_copy, theta_threshold, top_k_indices, ugly_outlier_candidates, feature_names):
    outlier_feature_indices = {}
    threshold = theta_threshold
    for column_indice in top_k_indices:
        select_column_data = data_copy[:, column_indice]
        sorted_indices = np.argsort(select_column_data)
        sorted_data = select_column_data[sorted_indices]
        # æ‰¾åˆ°Aå±æ€§ä¸‹çš„æ‰€æœ‰å¼‚å¸¸å€¼
        outliers = []
        outliers_index = []
        # æ£€æŸ¥åˆ—è¡¨é¦–å°¾å…ƒç´ 
        if len(sorted_data) > 1:
            if (sorted_data[1] - sorted_data[0] >= threshold):
                outliers.append(sorted_data[0])
                outliers_index.append(sorted_indices[0])
            if (sorted_data[-1] - sorted_data[-2] >= threshold):
                outliers.append(sorted_data[-1])
                outliers_index.append(sorted_indices[-1])
        # æ£€æŸ¥ä¸­é—´å…ƒç´ 
        for i in range(1, len(sorted_data) - 1):
            current_value = sorted_data[i]
            left_value = sorted_data[i - 1]
            right_value = sorted_data[i + 1]
            if (current_value - left_value >= threshold) and (right_value - current_value >= threshold):
                outliers.append(current_value)
                outliers_index.append(sorted_indices[i])
        intersection = np.intersect1d(np.array(outliers_index), ugly_outlier_candidates)
        outlier_feature_indices[column_indice] = intersection
        outlier_tuple_set = set()
        for value in outlier_feature_indices.values():
            outlier_tuple_set.update(value)
        X_copy_repair_indices = list(outlier_tuple_set)
        return X_copy_repair_indices

# section loss(M,D,ğ‘¡,ğ´)çš„å®ç°
def loss(clf, X_copy, X_test_copy, y_train, loss_choice):

    # choice ä½¿ç”¨sklearnåº“ä¸­çš„hingeæŸå¤±å‡½æ•°
    if loss_choice == "hinge":
        decision_values = clf.decision_function(X_copy)
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„hingeæŸå¤±
        num_samples = X_copy.shape[0]
        num_classes = clf.classes_.shape[0]
        hinge_losses = np.zeros((num_samples, num_classes))
        hinge_loss = np.zeros(num_samples)
        for i in range(num_samples):
            correct_class = int(y[i])
            for j in range(num_classes):
                if j != correct_class:
                    loss_j = max(0, 1 - decision_values[i, correct_class] + decision_values[i, j])
                    hinge_losses[i, j] = loss_j
            hinge_loss[i] = np.max(hinge_losses[i])
        # è®¡ç®— hinge_loss çš„å‡å€¼
        mean_hinge_loss = np.mean(hinge_loss)
        print("æ‰€æœ‰æ ·æœ¬çš„hinge lossçš„å‡å€¼ï¼š", mean_hinge_loss)
        # åœ¨æ‰€æœ‰åŠ å™ªæ•°æ®Dä¸­æŸå¤±å‡½æ•°é«˜äºé˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•
        ugly_outlier_candidates = np.where(hinge_loss > 1)[0]

    # choice ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°(é€‚ç”¨äºäºŒåˆ†ç±»æ•°æ®é›†ä¸‹çš„é€šç”¨åˆ†ç±»å™¨ï¼Œåˆ¤å®šbadä¸å¤Ÿå‡†ç¡®)
    elif loss_choice == "cross_entropy":
        # è·å–å†³ç­–å€¼
        decision_values = clf.decision_function(X_copy)
        # å°†å†³ç­–å€¼è½¬æ¢ä¸ºé€‚ç”¨äº Softmax çš„äºŒç»´æ•°ç»„
        decision_values_reshaped = decision_values.reshape(-1, 1)  # å˜æˆ (n_samples, 1)
        # åº”ç”¨ Softmax å‡½æ•°ï¼ˆå¯ä»¥æ‰‹åŠ¨å®ç°æˆ–ä½¿ç”¨ scipyï¼‰
        y_pred = softmax(np.hstack((decision_values_reshaped, -decision_values_reshaped)), axis=1)
        # åˆ›å»º OneHotEncoder å®ä¾‹
        encoder = OneHotEncoder(sparse=False)
        # é¢„æµ‹y_testçš„å€¼ï¼Œå¹¶ä¸y_trainç»„åˆæˆä¸ºy_ground
        y_test_pred = clf.predict(X_test_copy)
        y_ground = np.hstack((y_train, y_test_pred))
        # å¯¹y_groundè¿›è¡Œç‹¬çƒ­ç¼–ç 
        y_true = encoder.fit_transform(y_ground.reshape(-1, 1))
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±
        loss_per_sample = -np.sum(y_true * np.log(y_pred + 1e-12), axis=1)
        # è®¡ç®—æµ‹è¯•é›†å¹³å‡å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤±
        average_loss = -np.mean(np.sum(y_true * np.log(y_pred + 1e-12), axis=1))
        bad_samples = np.where(loss_per_sample > average_loss)[0]
        good_samples = np.where(loss_per_sample <= average_loss)[0]
        # åœ¨æ‰€æœ‰åŠ å™ªæ•°æ®Dä¸­æŸå¤±å‡½æ•°é«˜äºé˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•
        # ugly_outlier_candidates = np.where(bad_samples > 1)[0]
        ugly_outlier_candidates = bad_samples

    # choice ä½¿ç”¨å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤±å‡½æ•°
    elif loss_choice == "multi_cross_entropy":
        # è·å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å†³ç­–å€¼ (logits)
        decision_values_train = clf.decision_function(X_copy)
        decision_values_test = clf.decision_function(X_test_copy)

        # å¯¹å†³ç­–å€¼åº”ç”¨ Softmaxï¼ˆscipy ä¸­å®ç°çš„ Softmaxï¼‰
        y_pred_train = softmax(decision_values_train, axis=1)
        y_pred_test = softmax(decision_values_test, axis=1)

        # åˆ›å»º OneHotEncoder å®ä¾‹
        encoder = OneHotEncoder(sparse=False)

        # å¯¹è®­ç»ƒé›†æ ‡ç­¾è¿›è¡Œç‹¬çƒ­ç¼–ç 
        y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))

        # é¢„æµ‹æµ‹è¯•é›†æ ‡ç­¾
        y_test_pred = clf.predict(X_test_copy)
        y_test_onehot = encoder.transform(y_test_pred.reshape(-1, 1))

        # è®¡ç®—è®­ç»ƒé›†çš„äº¤å‰ç†µæŸå¤± per sample
        loss_per_sample_train = -np.sum(y_train_onehot * np.log(y_pred_train + 1e-12), axis=1)

        # è®¡ç®—æµ‹è¯•é›†çš„äº¤å‰ç†µæŸå¤± per sample
        loss_per_sample_test = -np.sum(y_test_onehot * np.log(y_pred_test + 1e-12), axis=1)

        # åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æŸå¤±
        loss_per_sample = np.concatenate([loss_per_sample_train, loss_per_sample_test])

        # è®¡ç®—å¹³å‡äº¤å‰ç†µæŸå¤±
        average_loss = np.mean(loss_per_sample)

        # æ‰¾å‡ºæŸå¤±è¾ƒå¤§çš„æ ·æœ¬ï¼Œä½œä¸ºâ€œåæ ·æœ¬â€
        bad_samples = np.where(loss_per_sample > average_loss)[0]
        good_samples = np.where(loss_per_sample <= average_loss)[0]

        # æ‰¾åˆ°æ‰€æœ‰æŸå¤±è¾ƒå¤§çš„æ ·æœ¬çš„ç´¢å¼•ï¼Œå¯èƒ½æ˜¯â€œç¦»ç¾¤æ ·æœ¬â€
        ugly_outlier_candidates = bad_samples

    # choice ç›´æ¥åˆ¤æ–­
    else:
        y_pred = clf.predict(X_copy)
        ugly_outlier_candidates = np.where(y_pred != y)[0]
        # æå–å¯¹åº”ç´¢å¼•çš„æ ‡ç­¾
        selected_labels = y[ugly_outlier_candidates]
        print("ugly_outlier_candidatesçš„æ•°é‡ï¼š", len(ugly_outlier_candidates))
        print("ugly_outlier_candidatesä¸­æ ‡ç­¾ä¸º1çš„æ ·æœ¬æ•°é‡ï¼š", np.sum(selected_labels == 1))

    return ugly_outlier_candidates

# section imbalanced(ğ·,ğ‘…,ğ‘¡,ğ´,ğ›¿)çš„å®ç°
def calculate_made(data_copy):
    median = np.median(data_copy)  # è®¡ç®—ä¸­ä½æ•°
    abs_deviation = np.abs(data_copy - median)  # è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸ä¸­ä½æ•°çš„ç»å¯¹è¯¯å·®
    mad = np.median(abs_deviation)  # è®¡ç®—ç»å¯¹è¯¯å·®å‡å€¼
    made = 1.843 * mad
    return median, made

def imbalanced(top_k_indices, delta_threshold, data, X_copy_repair_indices, feature_names):
    imbalanced_tuple_indices = set()

    # åˆå§‹åŒ–MinMaxScaler
    scaler_new = MinMaxScaler()
    data_imbalance = data
    # _, file_extension = os.path.splitext(file_path)
    # # åˆ¤æ–­æ–‡ä»¶æ‰©å±•å
    # if file_extension.lower() == '.xlsx':
    #     data_imbalance = pd.read_excel(file_path)
    # else:
    #     data_imbalance = pd.read_csv(file_path, error_bad_lines=False, warn_bad_lines=False)


    if len(data_imbalance) > 20000:
        data_imbalance = data_imbalance.sample(n=20000, random_state=42)

    # æ£€æµ‹éæ•°å€¼åˆ—
    data_imbalance_df = pd.DataFrame(data_imbalance, columns=feature_names[:-1])
    non_numeric_columns = data_imbalance_df.select_dtypes(exclude=[np.number]).columns
    # é€‰æ‹©æ•°å€¼åˆ—
    numeric_columns = data_imbalance_df.select_dtypes(include=[np.number]).columns

    # ä¸ºæ¯ä¸ªéæ•°å€¼åˆ—åˆ›å»ºä¸€ä¸ª LabelEncoder å®ä¾‹
    encoders = {}
    for column in non_numeric_columns:
        encoder = LabelEncoder()
        data_imbalance[column] = encoder.fit_transform(data_imbalance[column])
        encoders[column] = encoder  # ä¿å­˜æ¯ä¸ªåˆ—çš„ç¼–ç å™¨ï¼Œä»¥ä¾¿å°†æ¥å¯èƒ½éœ€è¦è§£ç 

    # å¯¹æ•°å€¼åˆ—è¿›è¡Œæ ‡å‡†åŒ–
    data_imbalance_df[numeric_columns] = scaler_new.fit_transform(data_imbalance_df[numeric_columns])

    # å°†æ ‡å‡†åŒ–åçš„ DataFrame è½¬å› numpy æ•°ç»„
    data_imbalance = data_imbalance_df.values

    for feature in top_k_indices:
        # å¯¹æ¯åˆ—æ•°æ®è¿›è¡Œåˆ†ç»„
        bins = np.arange(0, 1.01, delta_threshold)  # ç”Ÿæˆ0-1ä¹‹é—´100ä¸ªé—´éš”çš„æ•°ç»„
        digitized = np.digitize(data_imbalance[:, feature], bins)
        # ç»Ÿè®¡æ¯ä¸ªåŒºé—´çš„è®¡æ•°
        unique_bins, counts = np.unique(digitized, return_counts=True)
        # è®¾ç½®æœ€å°æ”¯æŒæ•°å·®å€¼
        median_imbalance, made_imbalance = calculate_made(counts)

        for t in X_copy_repair_indices:
            train_row_number = X_train.shape[0]
            ta = data_imbalance[t, feature]
            # æ‰¾åˆ° ta æ‰€åœ¨çš„é—´éš”
            ta_bin = np.digitize([ta], bins)[0]
            # æ‰¾åˆ° ta æ‰€åœ¨é—´éš”çš„è®¡æ•°
            ta_count = counts[unique_bins == ta_bin][0]
            lower_threshold = median_imbalance - 2 * made_imbalance
            upper_threshold = median_imbalance + 2 * made_imbalance
            if ta_count < lower_threshold or ta_count > upper_threshold:
                imbalanced_tuple_indices.add(t)

    X_copy_repair_imbalanced_indices = list(imbalanced_tuple_indices)

    return X_copy_repair_imbalanced_indices


def calculate_accuracy(ugly_outlier_index, true_ugly_indices, data_length):
    # åˆå§‹åŒ–æ‰€æœ‰ç´¢å¼•ä¸º0 (è¡¨ç¤ºä¸æ˜¯ä¸‘é™‹ç¦»ç¾¤å€¼)
    y_pred = np.zeros(data_length, dtype=int)
    y_true = np.zeros(data_length, dtype=int)
    # ugly_outlier_index = np.array(list(ugly_outlier_index), dtype=int)
    # true_ugly_indices = np.array(true_ugly_indices, dtype=int)

    # å°†é¢„æµ‹çš„ä¸‘é™‹ç¦»ç¾¤å€¼æ ‡è®°ä¸º1
    y_pred[ugly_outlier_index] = 1
    # å°†çœŸå®çš„ä¸‘é™‹ç¦»ç¾¤å€¼æ ‡è®°ä¸º1
    y_true[true_ugly_indices] = 1

    # è®¡ç®—å‡†ç¡®åº¦
    accuracy = accuracy_score(y_true, y_pred)
    return accuracy

# section è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–
def objective(trial):
    # é€‰æ‹©è¶…å‚æ•°
    need_MDetI = trial.suggest_int("need_MDetI", low=0, high=1)
    need_MDetO = trial.suggest_int("need_MDetO", low=0, high=1)
    need_outlier = trial.suggest_int("need_outlier", low=0, high=1)
    need_loss = trial.suggest_int("need_loss", low=0, high=1)
    need_imbalanced = trial.suggest_int("need_imbalanced", low=0, high=1)
    outlier_detector = trial.suggest_categorical("outlier_detector", ["GOAD", "RCA", "SLAD"])
    theta_threshold = trial.suggest_float("theta_threshold", low=0.001, high=0.01, step=0.001)
    loss_choice = trial.suggest_categorical("loss_choice", ["hinge"])
    delta_threshold = trial.suggest_float("delta_threshold", low=0.001, high=0.01, step=0.001)

    # Outlier Detection: å¯¹è®­ç»ƒé›†å’ŒéªŒè¯é›†è¿›è¡Œå¤„ç†
    if need_MDetO == 1:
        _, _, outliers_index_noise = MDetO(outlier_detector, X_train, X_val, X_test)  # ä¿®æ”¹ä¸ºä½¿ç”¨è®­ç»ƒé›†å’ŒéªŒè¯é›†
    else:
        outliers_index_noise = list(range(len(X_train)))  # ä½¿ç”¨è®­ç»ƒé›†çš„ç´¢å¼•

    # ç‰¹å¾é€‰æ‹©: æ ¹æ® `need_MDetI` çš„å€¼é€‰æ‹©ç‰¹å¾
    if need_MDetI == 1:
        _, top_k_indices = MDetI(svm_model_noise, X_train, X_val, feature_names)  # ä¿®æ”¹ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    else:
        top_k_indices = list(range(X_train.shape[1] - 1))  # ä½¿ç”¨è®­ç»ƒé›†çš„ç‰¹å¾ç´¢å¼•

    # æŸå¤±å¤„ç†: æ ¹æ® `need_loss` é€‰æ‹©æŸå¤±æ•°æ®
    if need_loss == 1:
        ugly_outlier_candidates = loss(svm_model_noise, X_train, X_test, y_train, loss_choice)  # ä½¿ç”¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    else:
        ugly_outlier_candidates = np.array(range(len(X_train)))  # é»˜è®¤è¿”å›æ‰€æœ‰è®­ç»ƒé›†çš„ç´¢å¼•

    # å¼‚å¸¸å€¼ä¿®å¤: æ ¹æ® `need_outlier` å’Œ `need_imbalanced` è¿›è¡Œä¿®å¤
    if need_outlier == 1:
        X_copy_repair_indices = outlier(X_train.copy(), theta_threshold, top_k_indices, ugly_outlier_candidates, feature_names)  # ä½¿ç”¨è®­ç»ƒé›†
    else:
        X_copy_repair_indices = list(range(len(X_train)))  # é»˜è®¤ä½¿ç”¨è®­ç»ƒé›†æ‰€æœ‰æ•°æ®

    if need_imbalanced == 1:
        X_copy_repair_imbalanced_indices = imbalanced(top_k_indices, delta_threshold, X_train.copy(), X_copy_repair_indices, feature_names)  # ä½¿ç”¨è®­ç»ƒé›†
        ugly_outlier_index = np.union1d(outliers_index_noise, X_copy_repair_imbalanced_indices)  # åˆå¹¶å¼‚å¸¸å€¼ç´¢å¼•
    else:
        ugly_outlier_index = outliers_index_noise

    # ä½¿ç”¨è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„é¢„æµ‹
    y_pred = svm_model_noise.predict(X_copy)
    true_ugly_indices = np.where(y_pred != y)[0]

    # è®¡ç®—é¢„æµ‹çš„å‡†ç¡®ç‡
    accuracy_noise = calculate_accuracy(ugly_outlier_index, true_ugly_indices, len(X_copy))
    return accuracy_noise


file_path = "./ugly_detection.log"
lock_obj = optuna.storages.journal.JournalFileOpenLock(file_path)

storage = optuna.storages.JournalStorage(
    optuna.storages.journal.JournalFileBackend(file_path, lock_obj=lock_obj),
)
# Execute an optimization by using an `Objective` instance.
study = optuna.create_study(storage=storage, direction="maximize")
study.optimize(objective, n_trials=5)

trial = study.best_trial
print("Objective Values: {}".format(trial.value))
print("Best hyperparameters: {}".format(trial.params))


