"""
ğ‘…(ğ‘¡) âˆ§ outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ) âˆ§ loss(M, D, ğ‘¡) > ğœ† âˆ§ Mğ‘ (ğ‘…, ğ´,M) â†’ ugly(ğ‘¡)
Rovaså¯¹ugly outliersçš„æ£€æµ‹èƒ½åŠ›
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
import torch
from deepod.models.tabular import GOAD

from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import KNNImputer
from lime.lime_tabular import LimeTabularExplainer
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import precision_recall_curve, auc
from sklearn.metrics import average_precision_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

# section æ ‡å‡†æ•°æ®é›†å¤„ç†

# subsection åŸå§‹çœŸå®æ•°æ®é›†ï¼ˆå¯¹åº”å®éªŒæµ‹è¯•1.1ï¼‰

# file_path = "../datasets/real_outlier/Cardiotocography.csv"
# file_path = "../datasets/real_outlier/annthyroid.csv"
# file_path = "../datasets/real_outlier/optdigits.csv"
# file_path = "../datasets/real_outlier/PageBlocks.csv"
# file_path = "../datasets/real_outlier/pendigits.csv"
# file_path = "../datasets/real_outlier/satellite.csv"
# file_path = "../datasets/real_outlier/shuttle.csv"
file_path = "../datasets/real_outlier/yeast.csv"

# subsection å«æœ‰ä¸åŒå¼‚å¸¸æ¯”ä¾‹çš„çœŸå®æ•°æ®é›†ï¼ˆå¯¹åº”å®éªŒæµ‹è¯•1.2ï¼‰

# choice Annthyroidæ•°æ®é›†
# file_path = "../datasets/real_outlier_varying_ratios/Annthyroid/Annthyroid_02_v01.csv"
# file_path = "../datasets/real_outlier_varying_ratios/Annthyroid/Annthyroid_05_v01.csv"
# file_path = "../datasets/real_outlier_varying_ratios/Annthyroid/Annthyroid_07.csv"

# choice Cardiotocographyæ•°æ®é›†
# file_path = "../datasets/real_outlier_varying_ratios/Cardiotocography/Cardiotocography_02_v01.csv"
# file_path = "../datasets/real_outlier_varying_ratios/Cardiotocography/Cardiotocography_05_v01.csv"
# file_path = "../datasets/real_outlier_varying_ratios/Cardiotocography/Cardiotocography_10_v01.csv"
# file_path = "../datasets/real_outlier_varying_ratios/Cardiotocography/Cardiotocography_20_v01.csv"
# file_path = "../datasets/real_outlier_varying_ratios/Cardiotocography/Cardiotocography_22.csv"

# choice PageBlocksæ•°æ®é›†
# file_path = "../datasets/real_outlier_varying_ratios/PageBlocks/PageBlocks_02_v01.csv"
# file_path = "../datasets/real_outlier_varying_ratios/PageBlocks/PageBlocks_05_v01.csv"

# choice Wiltæ•°æ®é›†
# file_path = "../datasets/real_outlier_varying_ratios/Wilt/Wilt_02_v01.csv"
# file_path = "../datasets/real_outlier_varying_ratios/Wilt/Wilt_05.csv"

# subsection å«æœ‰ä¸åŒå¼‚å¸¸ç±»å‹å’Œå¼‚å¸¸æ¯”ä¾‹çš„åˆæˆæ•°æ®é›†ï¼ˆä»çœŸå®æ•°æ®ä¸­åŠ å…¥ä¸åŒå¼‚å¸¸ç±»å‹åˆæˆï¼‰ï¼ˆå¯¹åº”å®éªŒæµ‹è¯•1.2ï¼‰

# choice Annthyroidæ•°æ®é›†+clusterå™ªå£°+ä¸åŒå™ªå£°æ¯”ä¾‹(æ•ˆæœç¨³å®š)
# file_path = "../datasets/synthetic_outlier/annthyroid_cluster_0.1.csv"
# file_path = "../datasets/synthetic_outlier/annthyroid_cluster_0.2.csv"
# file_path = "../datasets/synthetic_outlier/annthyroid_cluster_0.3.csv"

# choice Cardiotocographyæ•°æ®é›†+localå™ªå£°+ä¸åŒå™ªå£°æ¯”ä¾‹(å¥½ç”¨)
# file_path = "../datasets/synthetic_outlier/Cardiotocography_local_0.1.csv"
# file_path = "../datasets/synthetic_outlier/Cardiotocography_local_0.2.csv"
# file_path = "../datasets/synthetic_outlier/Cardiotocography_local_0.3.csv"

# choice PageBlocksæ•°æ®é›†+globalå™ªå£°+ä¸åŒå™ªå£°æ¯”ä¾‹(æ•ˆæœç¨³å®š)
# file_path = "../datasets/synthetic_outlier/PageBlocks_global_0.1.csv"
# file_path = "../datasets/synthetic_outlier/PageBlocks_global_0.2.csv"
# file_path = "../datasets/synthetic_outlier/PageBlocks_global_0.3.csv"

# choice satelliteæ•°æ®é›†+localå™ªå£°+ä¸åŒå™ªå£°æ¯”ä¾‹(å¥½ç”¨)
# file_path = "../datasets/synthetic_outlier/satellite_0.1.csv"
# file_path = "../datasets/synthetic_outlier/satellite_0.2.csv"
# file_path = "../datasets/synthetic_outlier/satellite_0.3.csv"

# choice annthyroidæ•°æ®é›†+localå™ªå£°+ä¸åŒå™ªå£°æ¯”ä¾‹(å¥½ç”¨)
# file_path = "../datasets/synthetic_outlier/annthyroid_0.1.csv"
# file_path = "../datasets/synthetic_outlier/annthyroid_0.2.csv"
# file_path = "../datasets/synthetic_outlier/annthyroid_0.3.csv"

# choice waveformæ•°æ®é›†+dependencyå™ªå£°+ä¸åŒå™ªå£°æ¯”ä¾‹
# file_path = "../datasets/synthetic_outlier/waveform_dependency_0.1.csv"
# file_path = "../datasets/synthetic_outlier/waveform_dependency_0.2.csv"
# file_path = "../datasets/synthetic_outlier/waveform_dependency_0.3.csv"

data = pd.read_csv(file_path)

# å¦‚æœæ•°æ®é‡è¶…è¿‡20000è¡Œï¼Œå°±éšæœºé‡‡æ ·åˆ°20000è¡Œ
if len(data) > 20000:
    data = data.sample(n=20000, random_state=42)

enc = LabelEncoder()
label_name = data.columns[-1]

# åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
data[label_name] = enc.fit_transform(data[label_name])

# æ£€æµ‹éæ•°å€¼åˆ—
non_numeric_columns = data.select_dtypes(exclude=[np.number]).columns

# ä¸ºæ¯ä¸ªéæ•°å€¼åˆ—åˆ›å»ºä¸€ä¸ª LabelEncoder å®ä¾‹
encoders = {}
for column in non_numeric_columns:
    encoder = LabelEncoder()
    data[column] = encoder.fit_transform(data[column])
    encoders[column] = encoder  # ä¿å­˜æ¯ä¸ªåˆ—çš„ç¼–ç å™¨ï¼Œä»¥ä¾¿å°†æ¥å¯èƒ½éœ€è¦è§£ç 

X = data.values[:, :-1]
y = data.values[:, -1]

# ç»Ÿè®¡ä¸åŒå€¼åŠå…¶æ•°é‡
unique_values, counts = np.unique(y, return_counts=True)

# è¾“å‡ºç»“æœ
for value, count in zip(unique_values, counts):
    print(f"æ ‡ç­¾: {value}, æ•°é‡: {count}")

# æ‰¾åˆ°æœ€å°æ ‡ç­¾çš„æ•°é‡
min_count = counts.min()
total_count = counts.sum()

# è®¡ç®—æ¯”ä¾‹
proportion = min_count / total_count
print(f"è¾ƒå°‘æ ‡ç­¾å æ®çš„æ¯”ä¾‹: {proportion:.4f}")
min_count_index = np.argmin(counts)  # æ‰¾åˆ°æœ€å°æ•°é‡çš„ç´¢å¼•
min_label = unique_values[min_count_index]  # å¯¹åº”çš„æ ‡ç­¾å€¼

all_columns = data.columns.values.tolist()
feature_names = all_columns[:-1]
class_name = all_columns[-1]

# æ‰¾åˆ°åˆ†ç±»ç‰¹å¾çš„åˆ—å
categorical_columns = data.select_dtypes(exclude=['float']).columns[:-1]
# è·å–åˆ†ç±»ç‰¹å¾å¯¹åº”çš„ç´¢å¼•
categorical_features = [data.columns.get_loc(col) for col in categorical_columns]

# section æ•°æ®ç‰¹å¾ç¼©æ”¾ä»¥åŠæ·»åŠ å™ªå£°
# section åˆ’åˆ†è®­ç»ƒé›†ï¼ŒéªŒè¯é›†ï¼Œæµ‹è¯•é›†ï¼Œåˆ’åˆ†æ¯”ä¾‹è‘³7;2:1

# æ•°æ®æ ‡å‡†åŒ–ï¼ˆå¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼‰
X = StandardScaler().fit_transform(X)

# è®°å½•åŸå§‹ç´¢å¼•
original_indices = np.arange(len(X))
# æŒ‰ç…§ 70% è®­ç»ƒé›†ï¼Œ20% éªŒè¯é›†ï¼Œ10% æµ‹è¯•é›†çš„æ¯”ä¾‹éšæœºåˆ’åˆ†æ•°æ®é›†
X_train, X_temp, y_train, y_temp, train_indices, temp_indices = train_test_split(X, y, original_indices, test_size=0.3, random_state=1)
# ä»ä¸´æ—¶æ•°æ®é›†ï¼ˆ30%ï¼‰ä¸­åˆ’åˆ†å‡º 10% æµ‹è¯•é›†å’Œ 20% éªŒè¯é›†
X_val, X_test, y_val, y_test, val_indices, test_indices = train_test_split(X_temp, y_temp, temp_indices, test_size=0.33, random_state=1)
print(f"Training Set: {X_train.shape}, Validation Set: {X_val.shape}, Test Set: {X_test.shape}")

# åŠ å…¥éšæœºå™ªå£°çš„æ¯”ä¾‹
noise_level = 0.2
# è®¡ç®—å™ªå£°æ•°é‡
n_samples = X.shape[0]
n_noise = int(noise_level * n_samples)
# éšæœºé€‰æ‹©è¦æ·»åŠ å™ªå£°çš„æ ·æœ¬
noise_indices = np.random.choice(n_samples, n_noise, replace=False)
# æ·»åŠ é«˜æ–¯å™ªå£°åˆ°ç‰¹å¾
X_copy = np.copy(X)
X_copy[noise_indices] += np.random.normal(0, 1, (n_noise, X.shape[1]))
# ä»åŠ å™ªæ•°æ®ä¸­ç”ŸæˆåŠ å™ªè®­ç»ƒæ•°æ®å’ŒåŠ å™ªæµ‹è¯•æ•°æ®
X_train_copy = X_copy[train_indices]
X_val_copy = X_copy[val_indices]
X_test_copy = X_copy[test_indices]
# åˆ›å»º DataFrame å­˜å‚¨åŠ å™ªæ•°æ®é›† D'ï¼ˆç”¨äºåç»­åˆ†æï¼‰
combined_array = np.hstack((X_copy, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
all_columns = list(data.columns)  # å‡è®¾æ‚¨å·²ç»æœ‰äº†æ•°æ®çš„åŸå§‹åˆ—å
data_copy = pd.DataFrame(combined_array, columns=all_columns)
# è®­ç»ƒé›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›† D ä¸­çš„ç´¢å¼•
train_noise = np.intersect1d(train_indices, noise_indices)
# æµ‹è¯•é›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›† D ä¸­çš„ç´¢å¼•
test_noise = np.intersect1d(test_indices, noise_indices)

# section æ‰¾åˆ°æœ‰å½±å“åŠ›çš„ç‰¹å¾ Mğ‘ (ğ‘…, ğ´, M)
# section æ²¡æš´éœ²æµ‹è¯•é›†
# choice LIME(Local Interpretable Model-Agnostic Explanation)(æ•ˆæœå¥½)
import re

i = len(feature_names)
np.random.seed(1)
categorical_names = {}
# å®šä¹‰ä¸€ä¸ªè¶…å‚æ•°ç½‘æ ¼
param_grid = {
    'n_estimators': [50, 100, 150],  # æ ‘çš„æ•°é‡
    'max_depth': [5, 10, 15, None],   # æ ‘çš„æœ€å¤§æ·±åº¦
    'min_samples_split': [2, 5, 10],  # å†…éƒ¨èŠ‚ç‚¹å†åˆ’åˆ†æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°
    'min_samples_leaf': [1, 2, 4],    # å¶èŠ‚ç‚¹çš„æœ€å°æ ·æœ¬æ•°
    'max_features': ['auto', 'sqrt', 'log2'],  # æœ€å¤§ç‰¹å¾æ•°
    'class_weight': ['balanced', None]  # ç±»åˆ«æƒé‡
}

# åˆå§‹åŒ–éšæœºæ£®æ—æ¨¡å‹
rf_model = RandomForestClassifier(random_state=42)

# åˆå§‹åŒ–GridSearchCVè¿›è¡Œè¶…å‚æ•°è°ƒä¼˜ï¼Œä½¿ç”¨éªŒè¯é›†è¿›è¡Œäº¤å‰éªŒè¯
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,
                           cv=3,  # 5æŠ˜äº¤å‰éªŒè¯
                           scoring='accuracy',  # ä½¿ç”¨å‡†ç¡®åº¦ä½œä¸ºè¯„åˆ†æŒ‡æ ‡
                           n_jobs=-1,  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸è¿›è¡Œå¹¶è¡Œè®¡ç®—
                           verbose=2)  # æ‰“å°è¯¦ç»†çš„è¿›åº¦ä¿¡æ¯

# åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œç½‘æ ¼æœç´¢
grid_search.fit(X_val, y_val)

# è¾“å‡ºæœ€ä½³å‚æ•°å’Œæœ€ä½³äº¤å‰éªŒè¯å¾—åˆ†
print("æœ€ä½³è¶…å‚æ•°ç»„åˆï¼š", grid_search.best_params_)
print("æœ€ä½³äº¤å‰éªŒè¯å‡†ç¡®åº¦ï¼š", grid_search.best_score_)

# ä½¿ç”¨æœ€ä½³è¶…å‚æ•°è®­ç»ƒæ¨¡å‹
rf_model_noise = grid_search.best_estimator_

rf_model_noise.fit(X_train_copy, y_train)

for feature in categorical_features:
    le = LabelEncoder()
    le.fit(data_copy.iloc[:, feature])
    data_copy.iloc[:, feature] = le.transform(data_copy.iloc[:, feature])
    categorical_names[feature] = le.classes_

explainer = LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_name,
                                                   categorical_features=categorical_features,
                                                   categorical_names=categorical_names, kernel_width=3)

predict_fn = lambda x: rf_model_noise.predict_proba(x)
exp = explainer.explain_instance(X_train[i], predict_fn, num_features=len(feature_names)//2)
# è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
top_features = exp.as_list()
top_feature_names = [re.search(r'([a-zA-Z_]\w*)', feature[0]).group(0).strip() for feature in top_features]
top_k_indices = [feature_names.index(name) for name in top_feature_names]
print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„å±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_indices))

# section æ‰¾åˆ°loss(M, D, ğ‘¡) > ğœ†çš„å…ƒç»„
# section æ²¡æš´éœ²æµ‹è¯•é›†

# choice ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°
from sklearn.preprocessing import OneHotEncoder
from scipy.special import softmax

y_mask = y.copy()
y_mask[test_indices] = rf_model_noise.predict(X_test_copy)
# å¯¹y_groundè¿›è¡Œç‹¬çƒ­ç¼–ç 
encoder = OneHotEncoder(sparse_output=False)
y_true = encoder.fit_transform(y.reshape(-1, 1))

# è·å–æ¯æ£µæ ‘çš„é¢„æµ‹æ¦‚ç‡ (n_samples, n_classes)
probabilities = rf_model_noise.predict_proba(X_copy)

# å‡è®¾æˆ‘ä»¬éœ€è¦å¯¹æ¯ä¸ªæ ·æœ¬åœ¨æ‰€æœ‰ç±»åˆ«ä¸Šçš„åˆ†æ•°åº”ç”¨ Softmax
# å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œéšæœºæ£®æ—ä¼šè¾“å‡ºå¤šä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œreshape ååº”ç”¨ Softmax
y_pred = softmax(probabilities, axis=1)

# è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±
loss_per_sample = -np.sum(y_true * np.log(y_pred + 1e-12), axis=1)
# ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
y_pred_copy = rf_model_noise.predict(X_copy)
# è®¡ç®—åˆ†ç±»é”™è¯¯ç‡
mis_classification_rate = 1 - accuracy_score(y, y_pred_copy)
bad_num = int(mis_classification_rate * len(X_copy))
# è®¡ç®—æµ‹è¯•é›†å¹³å‡å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤±
average_loss = -np.mean(np.sum(y_true * np.log(y_pred + 1e-12), axis=1))
# è·å–å·®è·æœ€å°çš„numä¸ªæ ·æœ¬çš„ç´¢å¼•
valid_samples_indices = np.where(loss_per_sample > average_loss)[0]
# è®¡ç®—è¿™äº›æ ·æœ¬ä¸average_lossçš„å·®è·
loss_difference = np.abs(loss_per_sample[valid_samples_indices] - average_loss)
# è·å–ä¸average_losså·®è·æœ€å°çš„numä¸ªæ ·æœ¬çš„ç´¢å¼•
bad_samples = valid_samples_indices[np.argsort(loss_difference)[-bad_num:]]
ugly_outlier_candidates = bad_samples

# è®¡ç®—æµ‹è¯•é›†å¹³å‡å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤±
# bad_samples = np.where(loss_per_sample > average_loss)[0]
# good_samples = np.where(loss_per_sample < average_loss)[0]
# ugly_outlier_candidates = bad_samples


# choice ä½¿ç”¨äºŒå…ƒhingeæŸå¤±å‡½æ•°
# y_mask = y.copy()
# y_mask[test_indices] = rf_model.predict(X_test_copy)
# predictions = rf_model.decision_function(X_copy)
# y_pred = np.where(predictions < 0, 0, 1)
# bad_samples = np.where(y_pred != y_mask)[0]
# ugly_outlier_candidates = bad_samples

# section è°“è¯outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )çš„å®ç°ï¼Œæ‰¾åˆ°æ‰€æœ‰æœ‰å½±å“åŠ›çš„ç‰¹å¾ä¸‹çš„å¼‚å¸¸å…ƒç»„
# section æ²¡æš´éœ²æµ‹è¯•é›†

outlier_feature_indices = {}
threshold = 0.15
for column_indice in top_k_indices:
    data_normalization = data_copy.copy()
    # åˆå§‹åŒ–ä¸€ä¸ª MinMaxScaler
    scaler = MinMaxScaler()
    # å¯¹ DataFrame è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ³¨æ„è¿™æ˜¯æŒ‰åˆ—æ“ä½œçš„
    data_normalization = pd.DataFrame(scaler.fit_transform(data_normalization), columns=data_normalization.columns)
    select_feature = feature_names[column_indice]
    # select_column_data = data_normalization[select_feature].loc[train_indices].values
    select_column_data = data_normalization[select_feature].values
    max_value = np.max(select_column_data)
    min_value = np.min(select_column_data)
    sorted_indices = np.argsort(select_column_data)
    sorted_data = select_column_data[sorted_indices]
    # æ‰¾åˆ°Aå±æ€§ä¸‹çš„æ‰€æœ‰å¼‚å¸¸å€¼
    outliers = []
    outliers_index = []
    # æ£€æŸ¥åˆ—è¡¨é¦–å°¾å…ƒç´ 
    if len(sorted_data) > 1:
        if (sorted_data[1] - sorted_data[0] >= threshold):
            outliers.append(sorted_data[0])
            outliers_index.append(sorted_indices[0])
        if (sorted_data[-1] - sorted_data[-2] >= threshold):
            outliers.append(sorted_data[-1])
            outliers_index.append(sorted_indices[-1])
    # æ£€æŸ¥ä¸­é—´å…ƒç´ 
    for i in range(1, len(sorted_data) - 1):
        current_value = sorted_data[i]
        left_value = sorted_data[i - 1]
        right_value = sorted_data[i + 1]
        if (current_value - left_value >= threshold) and (right_value - current_value >= threshold):
            outliers.append(current_value)
            outliers_index.append(sorted_indices[i])
    outliers_index_numpy = np.array(outliers_index)
    intersection = np.array(outliers_index)
    # intersection = np.intersect1d(np.array(outliers_index), ugly_outlier_candidates)
    # print("æœ‰å½±å“åŠ›çš„ç‰¹å¾Aä¸‹åŒæ—¶æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )å’Œloss(M, D, ğ‘¡) > ğœ†çš„æ‰€æœ‰å¼‚å¸¸å€¼ç´¢å¼•ä¸ºï¼š", intersection)
    outlier_feature_indices[column_indice] = intersection
# print(outlier_feature_indices)

# section ç¡®å®šæ•°æ®ä¸­çš„ugly outliers
# section æ²¡æš´éœ²æµ‹è¯•é›†

outlier_tuple_set = set()
for value in outlier_feature_indices.values():
    outlier_tuple_set.update(value)
outlier_tuple_set.update(bad_samples)
X_copy_repair_indices = list(outlier_tuple_set)
X_copy_repair = X_copy[X_copy_repair_indices]
y_repair = y[X_copy_repair_indices]

# ç”Ÿæˆä¿ç•™çš„è¡Œç´¢å¼•
rows_to_keep = np.setdiff1d(np.arange(X_copy.shape[0]), X_copy_repair_indices)

# ä½¿ç”¨ä¿ç•™çš„è¡Œç´¢å¼•é€‰æ‹©D'ä¸­çš„æ­£å¸¸æ•°æ®
# æ— éœ€ä¿®å¤çš„ç‰¹å¾å’Œæ ‡ç­¾å€¼
X_copy_inners = X_copy[rows_to_keep]
y_inners = y[rows_to_keep]

# section è®­ç»ƒä¸‹æ¸¸ä»»åŠ¡çš„random_forestæ¨¡å‹
# section æ²¡æš´éœ²æµ‹è¯•é›†

# subsection åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„random_forestæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹
# subsection æ²¡æš´éœ²æµ‹è¯•é›†

print("*" * 100)
rf_model = grid_search.best_estimator_
rf_model.fit(X_train, y_train)
train_label_pred = rf_model.predict(X_train)

# è®­ç»ƒæ ·æœ¬ä¸­è¢«random_forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices = np.where(y_train != rf_model.predict(X_train))[0]
print("è®­ç»ƒæ ·æœ¬ä¸­è¢«random_forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))

# æµ‹è¯•æ ·æœ¬ä¸­è¢«random_forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices = np.where(y_test != rf_model.predict(X_test))[0]
print("æµ‹è¯•æ ·æœ¬ä¸­è¢«random_forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))

# æ•´ä½“æ•°æ®é›†Dä¸­è¢«random_forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«random_forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š", (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))

# subsection åŠ å™ªæ•°æ®é›†ä¸Šè®­ç»ƒçš„random_forestæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹
# subsection æ²¡æš´éœ²æµ‹è¯•é›†

print("*" * 100)
train_label_pred_noise = rf_model_noise.predict(X_train_copy)

# åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«random_forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices_noise = np.where(y_train != rf_model_noise.predict(X_train_copy))[0]
print("åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«random_forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»åŠ å™ªè®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices_noise)/len(y_train))

# åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«random_forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices_noise = np.where(y_test != rf_model_noise.predict(X_test_copy))[0]
print("åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«random_forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices_noise)/len(y_test))

# æ•´ä½“åŠ å™ªæ•°æ®é›†Dä¸­è¢«random_forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«random_forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š", (len(wrong_classified_train_indices_noise) + len(wrong_classified_test_indices_noise))/(len(y_train) + len(y_test)))

# section å…¨éƒ¨åŠ å™ªæ•°æ®ä¸­è¢«random foreståˆ†ç±»å™¨è¯¯åˆ†ç±»çš„æ•°é‡
# section æ²¡æš´éœ²æµ‹è¯•é›†

label_pred = rf_model_noise.predict(X_copy)
wrong_classify_indices = []
for i in range(len(X_copy)):
    if y[i] != label_pred[i]:
        wrong_classify_indices.append(i)
print("è¢«è¯¯åˆ†ç±»çš„æ ·æœ¬æ•°é‡ï¼š", len(wrong_classify_indices))

# section æ£€æµ‹ugly outliersçš„å¬å›ç‡
# section æ²¡æš´éœ²æµ‹è¯•é›†

# ugly_found_by_detector = list(set(X_copy_repair_indices) & set(wrong_classify_indices))
ugly_found_by_detector = list(set(X_copy_repair_indices) & set(wrong_classify_indices))
print("å¬å›çš„ugly outliersçš„æ•°é‡ï¼š", len(ugly_found_by_detector))
print("ugly outliersçš„å¬å›ç‡ä¸ºï¼š", len(ugly_found_by_detector)/len(wrong_classify_indices))

# section é‡æ–°è®¡ç®—recall/precision/F1åˆ†æ•°
# è®¡ç®— TP, FN, FP
TP = len(set(X_copy_repair_indices) & set(wrong_classify_indices))  # äº¤é›†å…ƒç´ æ•°é‡
FN = len(set(wrong_classify_indices) - set(X_copy_repair_indices))  # s2ä¸­æœ‰ä½†s1ä¸­æ²¡æœ‰çš„å…ƒç´ æ•°é‡
FP = len(set(X_copy_repair_indices) - set(wrong_classify_indices))  # s1ä¸­æœ‰ä½†s2ä¸­æ²¡æœ‰çš„å…ƒç´ æ•°é‡

# è®¡ç®—å¬å›ç‡ (Recall)
Recall = TP / (TP + FN) if (TP + FN) != 0 else 0

# è®¡ç®—ç²¾ç¡®åº¦ (Precision)
Precision = TP / (TP + FP) if (TP + FP) != 0 else 0

# è®¡ç®— F1 åˆ†æ•°
F1 = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) != 0 else 0

# æ‰“å°ç»“æœ
print("*"*100)
print("å€™é€‰çš„ugly outliersåˆ—è¡¨é•¿åº¦ä¸ºï¼š", len(X_copy_repair_indices))
print("çœŸå®çš„ugly outliersåˆ—è¡¨é•¿åº¦ä¸ºï¼š", len(wrong_classify_indices))
print(f"Recall: {Recall:.4f}")
print(f"Precision: {Precision:.4f}")
print(f"F1 Score: {F1:.4f}")