"""
å‘ç°æ£€æµ‹ugly outliers çš„RoDsè§„åˆ™
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
import torch

from deepod.models.tabular import GOAD
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import KNNImputer
from lime.lime_tabular import LimeTabularExplainer
from deepod.models.tabular import DeepSVDD
from deepod.models.tabular import RCA
from deepod.models import REPEN, SLAD, ICL, NeuTraL
import re
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import precision_recall_curve, auc
from sklearn.metrics import average_precision_score
from sklearn.preprocessing import OneHotEncoder
from scipy.special import softmax
from sklearn.preprocessing import MinMaxScaler
import optuna
import os
import time
from sklearn.model_selection import GridSearchCV

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

# section æ•°æ®é¢„å¤„ç†
# file_path = "../large_datasets/sampled_flights.csv"
# data = pd.read_csv(file_path)
#
# if len(data) > 30000:
#     data = data.sample(n=30000, random_state=42)
#
# # åˆ é™¤åŒ…å«ä»»ä½• NaN çš„åˆ—
# data = data.dropna(axis=1)
file_path = "../../large_datasets/samples/flights_10000.csv"
data = pd.read_csv(file_path)

enc = LabelEncoder()
label_name = data.columns[-1]

# åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
data[label_name] = enc.fit_transform(data[label_name])

# æ£€æµ‹éæ•°å€¼åˆ—
non_numeric_columns = data.select_dtypes(exclude=[np.number]).columns

# ä¸ºæ¯ä¸ªéæ•°å€¼åˆ—åˆ›å»ºä¸€ä¸ª LabelEncoder å®ä¾‹
encoders = {}
for column in non_numeric_columns:
    encoder = LabelEncoder()
    data[column] = encoder.fit_transform(data[column])
    encoders[column] = encoder  # ä¿å­˜æ¯ä¸ªåˆ—çš„ç¼–ç å™¨ï¼Œä»¥ä¾¿å°†æ¥å¯èƒ½éœ€è¦è§£ç 

X = data.values[:, :-1]
y = data.values[:, -1]

# æ‰¾åˆ°åˆ†ç±»ç‰¹å¾çš„åˆ—å
categorical_columns = data.select_dtypes(exclude=['float']).columns[:-1]
# è·å–åˆ†ç±»ç‰¹å¾å¯¹åº”çš„ç´¢å¼•
categorical_features = [data.columns.get_loc(col) for col in categorical_columns]

# # ç»Ÿè®¡ä¸åŒå€¼åŠå…¶æ•°é‡
# unique_values, counts = np.unique(y, return_counts=True)
#
# # è¾“å‡ºç»“æœ
# for value, count in zip(unique_values, counts):
#     print(f"æ ‡ç­¾: {value}, æ•°é‡: {count}")
#
# # æ‰¾åˆ°æœ€å°æ ‡ç­¾çš„æ•°é‡
# min_count = counts.min()
# total_count = counts.sum()
#
# # è®¡ç®—æ¯”ä¾‹
# proportion = min_count / total_count
# print(f"è¾ƒå°‘æ ‡ç­¾å æ®çš„æ¯”ä¾‹: {proportion:.4f}")
# min_count_index = np.argmin(counts)  # æ‰¾åˆ°æœ€å°æ•°é‡çš„ç´¢å¼•
# min_label = unique_values[min_count_index]  # å¯¹åº”çš„æ ‡ç­¾å€¼

# section æ•°æ®ç‰¹å¾ç¼©æ”¾

# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)
# è®°å½•åŸå§‹ç´¢å¼•
original_indices = np.arange(len(X))
X_train, X_test, y_train, y_test, train_indices, test_indices = \
    train_test_split(X, y, original_indices, test_size=0.3, random_state=42)

# ç¬¬äºŒæ¬¡åˆ†å‰²ï¼šä»è®­ç»ƒé›†ä¸­å†æŠ½ 20% åšéªŒè¯é›†
_, X_val, _, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# åŠ å…¥éšæœºå™ªå£°çš„æ¯”ä¾‹
noise_level = 0.2
# è®¡ç®—å™ªå£°æ•°é‡
n_samples = X.shape[0]
n_noise = int(noise_level * n_samples)
# éšæœºé€‰æ‹©è¦æ·»åŠ å™ªå£°çš„æ ·æœ¬
noise_indices = np.random.choice(n_samples, n_noise, replace=False)
# æ·»åŠ é«˜æ–¯å™ªå£°åˆ°ç‰¹å¾
X_copy = np.copy(X)
X_copy[noise_indices] += np.random.normal(0, 1, (n_noise, X.shape[1]))
# ä»å«å™ªæ•°æ®ä¸­ç”Ÿæˆè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
X_train_copy = X_copy[train_indices]
X_test_copy = X_copy[test_indices]
feature_names = data.columns.values.tolist()
combined_array = np.hstack((X_copy, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
# æ·»åŠ å™ªå£°åçš„æ•°æ®é›†D'å¯¹åº”çš„Dataframe
data_copy = pd.DataFrame(combined_array, columns=feature_names)
# è®­ç»ƒé›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
train_noise = np.intersect1d(train_indices, noise_indices)
# æµ‹è¯•é›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
test_noise = np.intersect1d(test_indices, noise_indices)

print(data_copy.index)

# SECTION SVMæ¨¡å‹çš„å®ç°

# # subsection åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„SVMæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

# subsection åŠ å™ªæ•°æ®é›†ä¸Šè®­ç»ƒçš„SVMæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

print("*" * 100)
# å®šä¹‰è¶…å‚æ•°ç½‘æ ¼
param_grid = {
    'C': [0.1, 1.0, 10],  # æ­£åˆ™åŒ–å‚æ•°
    'kernel': ['linear', 'rbf'],  # æ ¸å‡½æ•°ç±»å‹
    'class_weight': ['balanced', None],  # ç±»åˆ«æƒé‡
    'gamma': ['scale', 'auto']  # å¯¹äº rbf æ ¸æœ‰ç”¨
}

# åˆå§‹åŒ– SVM æ¨¡å‹ï¼ˆå¸¦æ¦‚ç‡ä¼°è®¡ï¼‰
svm_model_noise = svm.SVC(probability=True, random_state=42)

# åˆå§‹åŒ– GridSearchCV è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
grid_search = GridSearchCV(estimator=svm_model_noise, param_grid=param_grid,
                           cv=3,
                           scoring='accuracy',
                           n_jobs=-1,
                           verbose=2)

# æ‹Ÿåˆæ¨¡å‹ï¼ˆåœ¨éªŒè¯é›†ä¸Šè¿›è¡Œç½‘æ ¼æœç´¢ï¼‰
grid_search.fit(X_val, y_val)

# è¾“å‡ºæœ€ä½³å‚æ•°å’Œäº¤å‰éªŒè¯å¾—åˆ†
print("æœ€ä½³è¶…å‚æ•°ç»„åˆï¼š", grid_search.best_params_)
print("æœ€ä½³äº¤å‰éªŒè¯å‡†ç¡®åº¦ï¼š", grid_search.best_score_)

# ä½¿ç”¨æœ€ä½³æ¨¡å‹
svm_model_noise = grid_search.best_estimator_
svm_model_noise.fit(X_train_copy, y_train)
train_label_pred_noise = svm_model_noise.predict(X_train_copy)
test_label_pred_noise = svm_model_noise.predict(X_test_copy)



# SECTION æ£€æµ‹æœ‰å½±å“åŠ›çš„ç‰¹å¾MDetO(ğ‘¡,ğ´,D)çš„å®ç°
# choice LIME(Local Interpretable Model-Agnostic Explanation)(æ•ˆæœå¥½)
def MDetI(clf, data_copy, X_train_copy):
    feature_names = data_copy.columns.values.tolist()

    # æ‰¾åˆ°åˆ†ç±»ç‰¹å¾çš„åˆ—å
    categorical_columns = data_copy.select_dtypes(exclude=['float']).columns[:-1]
    # è·å–åˆ†ç±»ç‰¹å¾å¯¹åº”çš„ç´¢å¼•
    categorical_features = [data_copy.columns.get_loc(col) for col in categorical_columns]

    i = len(feature_names)
    np.random.seed(1)
    categorical_names = {}
    for feature in categorical_features:
        le = LabelEncoder()
        le.fit(data_copy.iloc[:, feature])
        data_copy.iloc[:, feature] = le.transform(data_copy.iloc[:, feature])
        categorical_names[feature] = le.classes_
    explainer = LimeTabularExplainer(X_train_copy, feature_names=feature_names, class_names=feature_names,
                                     categorical_features=categorical_features,
                                     categorical_names=categorical_names, kernel_width=3)
    # predict_proba æ–¹æ³•ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œpredict æ–¹æ³•ç”¨äºå›å½’ä»»åŠ¡
    predict_fn = lambda x: clf.predict_proba(x)
    exp = explainer.explain_instance(X_train_copy[i], predict_fn, num_features=len(feature_names) // 2)
    # è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
    top_features = exp.as_list()
    top_feature_names = [re.search(r'([a-zA-Z_]\w*)', feature[0]).group(0).strip() for feature in top_features]
    top_k_indices = [feature_names.index(name) for name in top_feature_names]
    return top_k_indices

# SECTION MDetO(ğ‘¡,ğ´,D) é’ˆå¯¹å…ƒç»„å¼‚å¸¸çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨GOADçš„å®ç°
def MDetO(outlier_detector, X_train_copy, X_test_copy, X_copy):
    epochs = 3
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    n_trans = 64
    random_state = 42

    if outlier_detector == "GOAD":
        out_clf_noise = GOAD(epochs=epochs, device=device, n_trans=n_trans)
        out_clf_noise.fit(X_train_copy, y=None)
    elif outlier_detector == "RCA":
        out_clf_noise = RCA(epochs=epochs, device=device, act='LeakyReLU')
        out_clf_noise.fit(X_train_copy, y=None)
    elif outlier_detector == "DeepSVDD":
        out_clf_noise = DeepSVDD(epochs=epochs, device=device, random_state=random_state)
        out_clf_noise.fit(X_train_copy, y=None)
    elif outlier_detector == "RePEN":
        out_clf_noise = REPEN(epochs=epochs, device=device)
        out_clf_noise.fit(X_train_copy, y=None)
    elif outlier_detector == "ICL":
        out_clf_noise = ICL(epochs=epochs, device=device, n_ensemble='auto')
        out_clf_noise.fit(X_train_copy, y=None)
    elif outlier_detector == "NeuTraL":
        out_clf_noise = NeuTraL(epochs=epochs, device=device)
        out_clf_noise.fit(X_train_copy, y=None)
    else:
        out_clf_noise = SLAD(epochs=epochs, device=device)
        out_clf_noise.fit(X_train_copy, y=None)

    # ä»åŠ å™ªè®­ç»ƒé›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•
    train_pred_labels_noise, train_confidence_noise = out_clf_noise.predict(X_train_copy, return_confidence=True)
    print("åŠ å™ªè®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf_noise.threshold_)
    train_outliers_index_noise = []
    print("åŠ å™ªè®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train_copy))
    for i in range(len(X_train_copy)):
        if train_pred_labels_noise[i] == 1:
            train_outliers_index_noise.append(i)
    print("åŠ å™ªè®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index_noise)

    # ä»åŠ å™ªæµ‹è¯•é›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•
    test_pred_labels_noise, test_confidence_noise = out_clf_noise.predict(X_test_copy, return_confidence=True)
    print("åŠ å™ªæµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf_noise.threshold_)
    test_outliers_index_noise = []
    print("åŠ å™ªæµ‹è¯•é›†æ ·æœ¬æ•°ï¼š", len(X_test_copy))
    for i in range(len(X_test_copy)):
        if test_pred_labels_noise[i] == 1:
            test_outliers_index_noise.append(i)
    # è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
    print("åŠ å™ªæµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", test_outliers_index_noise)

    # ä»æ•´ä½“çš„åŠ å™ªæ•°æ®é›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•
    pred_labels_noise, onfidence_noise = out_clf_noise.predict(X_copy, return_confidence=True)
    print("åŠ å™ªæ•°æ®é›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf_noise.threshold_)
    outliers_index_noise = []
    print("åŠ å™ªæ•°æ®é›†æ ·æœ¬æ•°ï¼š", len(X_copy))
    for i in range(len(X_copy)):
        if pred_labels_noise[i] == 1:
            outliers_index_noise.append(i)
    # è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
    print("åŠ å™ªæ•°æ®é›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", outliers_index_noise)

    return train_outliers_index_noise, test_outliers_index_noise, outliers_index_noise

# section outlier(ğ·,ğ‘…,ğ‘¡,ğ´,ğœƒ)çš„å®ç°
def outlier(data_copy, theta_threshold, top_k_indices, ugly_outlier_candidates):
    outlier_feature_indices = {}
    threshold = theta_threshold
    feature_names = data_copy.columns.values.tolist()
    for column_indice in top_k_indices:
        select_feature = feature_names[column_indice]
        select_column_data = data_copy[select_feature].values
        sorted_indices = np.argsort(select_column_data)
        sorted_data = select_column_data[sorted_indices]
        # æ‰¾åˆ°Aå±æ€§ä¸‹çš„æ‰€æœ‰å¼‚å¸¸å€¼
        outliers = []
        outliers_index = []
        # æ£€æŸ¥åˆ—è¡¨é¦–å°¾å…ƒç´ 
        if len(sorted_data) > 1:
            if (sorted_data[1] - sorted_data[0] >= threshold):
                outliers.append(sorted_data[0])
                outliers_index.append(sorted_indices[0])
            if (sorted_data[-1] - sorted_data[-2] >= threshold):
                outliers.append(sorted_data[-1])
                outliers_index.append(sorted_indices[-1])
        # æ£€æŸ¥ä¸­é—´å…ƒç´ 
        for i in range(1, len(sorted_data) - 1):
            current_value = sorted_data[i]
            left_value = sorted_data[i - 1]
            right_value = sorted_data[i + 1]
            if (current_value - left_value >= threshold) and (right_value - current_value >= threshold):
                outliers.append(current_value)
                outliers_index.append(sorted_indices[i])
        intersection = np.intersect1d(np.array(outliers_index), ugly_outlier_candidates)
        outlier_feature_indices[column_indice] = intersection
        outlier_tuple_set = set()
        for value in outlier_feature_indices.values():
            outlier_tuple_set.update(value)
        X_copy_repair_indices = list(outlier_tuple_set)
        return X_copy_repair_indices

# section loss(M,D,ğ‘¡,ğ´)çš„å®ç°
def loss(clf, X_copy, X_test_copy, y_train, loss_choice):

    # choice ä½¿ç”¨sklearnåº“ä¸­çš„hingeæŸå¤±å‡½æ•°
    if loss_choice == "hinge":
        decision_values = clf.decision_function(X_copy)
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„hingeæŸå¤±
        num_samples = X_copy.shape[0]
        num_classes = clf.classes_.shape[0]
        hinge_losses = np.zeros((num_samples, num_classes))
        hinge_loss = np.zeros(num_samples)
        for i in range(num_samples):
            correct_class = int(y[i])
            for j in range(num_classes):
                if j != correct_class:
                    loss_j = max(0, 1 - decision_values[i, correct_class] + decision_values[i, j])
                    hinge_losses[i, j] = loss_j
            hinge_loss[i] = np.max(hinge_losses[i])
        # è®¡ç®— hinge_loss çš„å‡å€¼
        mean_hinge_loss = np.mean(hinge_loss)
        print("æ‰€æœ‰æ ·æœ¬çš„hinge lossçš„å‡å€¼ï¼š", mean_hinge_loss)
        # åœ¨æ‰€æœ‰åŠ å™ªæ•°æ®Dä¸­æŸå¤±å‡½æ•°é«˜äºé˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•
        ugly_outlier_candidates = np.where(hinge_loss > 1)[0]

    # choice ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°(é€‚ç”¨äºäºŒåˆ†ç±»æ•°æ®é›†ä¸‹çš„é€šç”¨åˆ†ç±»å™¨ï¼Œåˆ¤å®šbadä¸å¤Ÿå‡†ç¡®)
    elif loss_choice == "cross_entropy":
        # è·å–å†³ç­–å€¼
        decision_values = clf.decision_function(X_copy)
        # å°†å†³ç­–å€¼è½¬æ¢ä¸ºé€‚ç”¨äº Softmax çš„äºŒç»´æ•°ç»„
        decision_values_reshaped = decision_values.reshape(-1, 1)  # å˜æˆ (n_samples, 1)
        # åº”ç”¨ Softmax å‡½æ•°ï¼ˆå¯ä»¥æ‰‹åŠ¨å®ç°æˆ–ä½¿ç”¨ scipyï¼‰
        y_pred = softmax(np.hstack((decision_values_reshaped, -decision_values_reshaped)), axis=1)
        # åˆ›å»º OneHotEncoder å®ä¾‹
        encoder = OneHotEncoder(sparse_output=False)
        # é¢„æµ‹y_testçš„å€¼ï¼Œå¹¶ä¸y_trainç»„åˆæˆä¸ºy_ground
        y_test_pred = clf.predict(X_test_copy)
        y_ground = np.hstack((y_train, y_test_pred))
        # å¯¹y_groundè¿›è¡Œç‹¬çƒ­ç¼–ç 
        y_true = encoder.fit_transform(y_ground.reshape(-1, 1))
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±
        loss_per_sample = -np.sum(y_true * np.log(y_pred + 1e-12), axis=1)
        # è®¡ç®—æµ‹è¯•é›†å¹³å‡å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤±
        average_loss = -np.mean(np.sum(y_true * np.log(y_pred + 1e-12), axis=1))
        bad_samples = np.where(loss_per_sample > average_loss)[0]
        good_samples = np.where(loss_per_sample <= average_loss)[0]
        # åœ¨æ‰€æœ‰åŠ å™ªæ•°æ®Dä¸­æŸå¤±å‡½æ•°é«˜äºé˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•
        # ugly_outlier_candidates = np.where(bad_samples > 1)[0]
        ugly_outlier_candidates = bad_samples

    # choice ä½¿ç”¨å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤±å‡½æ•°
    elif loss_choice == "multi_cross_entropy":
        # è·å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å†³ç­–å€¼ (logits)
        decision_values_train = clf.decision_function(X_copy)
        decision_values_test = clf.decision_function(X_test_copy)

        # å¯¹å†³ç­–å€¼åº”ç”¨ Softmaxï¼ˆscipy ä¸­å®ç°çš„ Softmaxï¼‰
        y_pred_train = softmax(decision_values_train, axis=1)
        y_pred_test = softmax(decision_values_test, axis=1)

        # åˆ›å»º OneHotEncoder å®ä¾‹
        encoder = OneHotEncoder(sparse_output=False)

        # å¯¹è®­ç»ƒé›†æ ‡ç­¾è¿›è¡Œç‹¬çƒ­ç¼–ç 
        y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))

        # é¢„æµ‹æµ‹è¯•é›†æ ‡ç­¾
        y_test_pred = clf.predict(X_test_copy)
        y_test_onehot = encoder.transform(y_test_pred.reshape(-1, 1))

        # è®¡ç®—è®­ç»ƒé›†çš„äº¤å‰ç†µæŸå¤± per sample
        loss_per_sample_train = -np.sum(y_train_onehot * np.log(y_pred_train + 1e-12), axis=1)

        # è®¡ç®—æµ‹è¯•é›†çš„äº¤å‰ç†µæŸå¤± per sample
        loss_per_sample_test = -np.sum(y_test_onehot * np.log(y_pred_test + 1e-12), axis=1)

        # åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æŸå¤±
        loss_per_sample = np.concatenate([loss_per_sample_train, loss_per_sample_test])

        # è®¡ç®—å¹³å‡äº¤å‰ç†µæŸå¤±
        average_loss = np.mean(loss_per_sample)

        # æ‰¾å‡ºæŸå¤±è¾ƒå¤§çš„æ ·æœ¬ï¼Œä½œä¸ºâ€œåæ ·æœ¬â€
        bad_samples = np.where(loss_per_sample > average_loss)[0]
        good_samples = np.where(loss_per_sample <= average_loss)[0]

        # æ‰¾åˆ°æ‰€æœ‰æŸå¤±è¾ƒå¤§çš„æ ·æœ¬çš„ç´¢å¼•ï¼Œå¯èƒ½æ˜¯â€œç¦»ç¾¤æ ·æœ¬â€
        ugly_outlier_candidates = bad_samples

    # choice ç›´æ¥åˆ¤æ–­
    else:
        y_pred = clf.predict(X_copy)
        ugly_outlier_candidates = np.where(y_pred != y)[0]
        # æå–å¯¹åº”ç´¢å¼•çš„æ ‡ç­¾
        selected_labels = y[ugly_outlier_candidates]
        print("ugly_outlier_candidatesçš„æ•°é‡ï¼š", len(ugly_outlier_candidates))
        print("ugly_outlier_candidatesä¸­æ ‡ç­¾ä¸º1çš„æ ·æœ¬æ•°é‡ï¼š", np.sum(selected_labels == 1))

    return ugly_outlier_candidates

# section imbalanced(ğ·,ğ‘…,ğ‘¡,ğ´,ğ›¿)çš„å®ç°
def calculate_made(data_copy):
    median = np.median(data_copy)  # è®¡ç®—ä¸­ä½æ•°
    abs_deviation = np.abs(data_copy - median)  # è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸ä¸­ä½æ•°çš„ç»å¯¹è¯¯å·®
    mad = np.median(abs_deviation)  # è®¡ç®—ç»å¯¹è¯¯å·®å‡å€¼
    made = 1.843 * mad
    return median, made

def imbalanced(top_k_indices, delta_threshold, data, X_copy_repair_indices):
    imbalanced_tuple_indices = set()

    # åˆå§‹åŒ–MinMaxScaler
    scaler_new = MinMaxScaler()
    data_imbalance = data
    # _, file_extension = os.path.splitext(file_path)
    # # åˆ¤æ–­æ–‡ä»¶æ‰©å±•å
    # if file_extension.lower() == '.xlsx':
    #     data_imbalance = pd.read_excel(file_path)
    # else:
    #     data_imbalance = pd.read_csv(file_path, error_bad_lines=False, warn_bad_lines=False)

    feature_names = data_imbalance.columns.values.tolist()

    # if len(data_imbalance) > 20000:
    #     data_imbalance = data_imbalance.sample(n=20000, random_state=42)

    # æ£€æµ‹éæ•°å€¼åˆ—
    non_numeric_columns = data_imbalance.select_dtypes(exclude=[np.number]).columns

    # ä¸ºæ¯ä¸ªéæ•°å€¼åˆ—åˆ›å»ºä¸€ä¸ª LabelEncoder å®ä¾‹
    encoders = {}
    for column in non_numeric_columns:
        encoder = LabelEncoder()
        data_imbalance[column] = encoder.fit_transform(data_imbalance[column])
        encoders[column] = encoder  # ä¿å­˜æ¯ä¸ªåˆ—çš„ç¼–ç å™¨ï¼Œä»¥ä¾¿å°†æ¥å¯èƒ½éœ€è¦è§£ç 

    data_imbalance[data_imbalance.columns] = scaler_new.fit_transform(data_imbalance[data_imbalance.columns])

    for feature in top_k_indices:
        select_feature = feature_names[feature]
        # å¯¹æ¯åˆ—æ•°æ®è¿›è¡Œåˆ†ç»„
        bins = np.arange(0, 1.01, delta_threshold)  # ç”Ÿæˆ0-1ä¹‹é—´100ä¸ªé—´éš”çš„æ•°ç»„
        digitized = np.digitize(data_imbalance[select_feature], bins)
        # ç»Ÿè®¡æ¯ä¸ªåŒºé—´çš„è®¡æ•°
        unique_bins, counts = np.unique(digitized, return_counts=True)
        # è®¾ç½®æœ€å°æ”¯æŒæ•°å·®å€¼
        median_imbalance, made_imbalance = calculate_made(counts)

        for t in X_copy_repair_indices:
            train_row_number = X_train.shape[0]
            ta = data_imbalance.iloc[t, feature]
            # æ‰¾åˆ° ta æ‰€åœ¨çš„é—´éš”
            ta_bin = np.digitize([ta], bins)[0]
            # æ‰¾åˆ° ta æ‰€åœ¨é—´éš”çš„è®¡æ•°
            ta_count = counts[unique_bins == ta_bin][0]
            lower_threshold = median_imbalance - 2 * made_imbalance
            upper_threshold = median_imbalance + 2 * made_imbalance
            if ta_count < lower_threshold or ta_count > upper_threshold:
                imbalanced_tuple_indices.add(t)

    X_copy_repair_imbalanced_indices = list(imbalanced_tuple_indices)

    return X_copy_repair_imbalanced_indices


def calculate_accuracy(ugly_outlier_index, true_ugly_indices, data_length):
    # åˆå§‹åŒ–æ‰€æœ‰ç´¢å¼•ä¸º0 (è¡¨ç¤ºä¸æ˜¯ä¸‘é™‹ç¦»ç¾¤å€¼)
    y_pred = np.zeros(data_length, dtype=int)
    y_true = np.zeros(data_length, dtype=int)
    # ugly_outlier_index = np.array(list(ugly_outlier_index), dtype=int)
    # true_ugly_indices = np.array(true_ugly_indices, dtype=int)

    # å°†é¢„æµ‹çš„ä¸‘é™‹ç¦»ç¾¤å€¼æ ‡è®°ä¸º1
    y_pred[ugly_outlier_index] = 1
    # å°†çœŸå®çš„ä¸‘é™‹ç¦»ç¾¤å€¼æ ‡è®°ä¸º1
    y_true[true_ugly_indices] = 1

    # è®¡ç®—å‡†ç¡®åº¦
    accuracy = accuracy_score(y_true, y_pred)
    return accuracy

# section è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–
def objective(trial):
    # need_MDetI = trial.suggest_int("need_MDetI", low=0, high=1)
    # need_MDetO = trial.suggest_int("need_MDetO", low=0, high=1)
    # need_outlier = trial.suggest_int("need_outlier", low=0, high=1)
    # need_loss = trial.suggest_int("need_loss", low=0, high=1)
    # need_imbalanced = trial.suggest_int("need_imbalanced", low=0, high=1)
    need_MDetI = 1
    # need_MDetO = trial.suggest_int("need_MDetO", low=0, high=1)
    need_MDetO = 1
    # need_outlier = trial.suggest_int("need_outlier", low=0, high=1)
    need_outlier = 1
    # need_loss = trial.suggest_int("need_loss", low=0, high=1)
    need_loss = 1
    # need_imbalanced = trial.suggest_int("need_imbalanced", low=0, high=1)
    need_imbalanced = 1
    loss_choice = trial.suggest_categorical("loss_choice", ["cross_entropy"])

    # # theta=3
    # outlier_detector = trial.suggest_categorical("outlier_detector", ["GOAD", "RCA", "SLAD"])
    # theta_threshold = trial.suggest_float("theta_threshold", low=0.001, high=0.009, step=0.004)
    # delta_threshold = trial.suggest_float("delta_threshold", low=0.001, high=0.009, step=0.004)

    # # theta=4
    # outlier_detector = trial.suggest_categorical("outlier_detector", ["GOAD", "RCA", "SLAD", "DeepSVDD"])
    # theta_threshold = trial.suggest_float("theta_threshold", low=0.001, high=0.01, step=0.003)
    # delta_threshold = trial.suggest_float("delta_threshold", low=0.001, high=0.01, step=0.003)

    # # theta=5
    # outlier_detector = trial.suggest_categorical("outlier_detector", ["GOAD", "RCA", "SLAD", "DeepSVDD", "RePEN"])
    # theta_threshold = trial.suggest_float("theta_threshold", low=0.001, high=0.009, step=0.002)
    # delta_threshold = trial.suggest_float("delta_threshold", low=0.001, high=0.009, step=0.002)

    # # theta=6
    # outlier_detector = trial.suggest_categorical("outlier_detector", ["GOAD", "RCA", "SLAD", "DeepSVDD", "RePEN", "ICL"])
    # theta_threshold = trial.suggest_float("theta_threshold", low=0.001, high=0.011, step=0.002)
    # delta_threshold = trial.suggest_float("delta_threshold", low=0.001, high=0.011, step=0.002)

    # theta=7
    outlier_detector = trial.suggest_categorical("outlier_detector", ["GOAD", "RCA", "SLAD", "DeepSVDD", "RePEN", "ICL", "NeuTraL"])
    theta_threshold = trial.suggest_float("theta_threshold", low=0.001, high=0.013, step=0.002)
    delta_threshold = trial.suggest_float("delta_threshold", low=0.001, high=0.013, step=0.002)

    if need_MDetO == 1:
        t_start_MDetO = time.time()
        _, _, outliers_index_noise = MDetO(outlier_detector, X_train_copy, X_test_copy, X_copy)
        t_end_MDetO = time.time()
        t_MDetO = t_end_MDetO - t_start_MDetO
    else:
        outliers_index_noise = list(range(len(data_copy)))
        t_MDetO = 0
    if need_MDetI == 1:
        t_start_MDetI = time.time()
        top_k_indices = MDetI(svm_model_noise, data_copy, X_train_copy)
        t_end_MDetI = time.time()
        t_MDetI = t_end_MDetI - t_start_MDetI
    else:
        top_k_indices = list(range(len(data_copy.columns) - 1))
        t_MDetI = 0
    if need_loss == 1:
        ugly_outlier_candidates = loss(svm_model_noise, X_copy, X_test_copy, y_train, loss_choice)
    else:
        ugly_outlier_candidates = np.array(range(len(data_copy)))
    if need_outlier ==1:
        t_start_outlier = time.time()
        X_copy_repair_indices = outlier(data_copy, theta_threshold, top_k_indices, ugly_outlier_candidates)
        t_end_outlier = time.time()
        t_outlier = t_end_outlier - t_start_outlier
    else:
        X_copy_repair_indices = list(range(len(data_copy)))
        t_outlier = 0
    if need_imbalanced == 1:
        t_start_imbalanced = time.time()
        X_copy_repair_imbalanced_indices = imbalanced(top_k_indices, delta_threshold, data_copy.copy(), X_copy_repair_indices)
        t_end_imbalanced = time.time()
        t_imbalanced = t_end_imbalanced - t_start_imbalanced
    else:
        X_copy_repair_imbalanced_indices = list(range(len(data_copy)))
        t_imbalanced = 0
    if need_imbalanced == 1:
        ugly_outlier_index = np.union1d(outliers_index_noise, X_copy_repair_imbalanced_indices)
    else:
        ugly_outlier_index = np.union1d(outliers_index_noise, X_copy_repair_imbalanced_indices)

    # ugly_outlier_index = np.array(set(outliers_index_noise) & set(X_copy_repair_indices) & set(X_copy_repair_imbalanced_indices))
    t_start_predicate = time.time()
    y_pred = svm_model_noise.predict(X_copy)
    t_end_predicate = time.time()
    t_predicate = t_end_predicate - t_start_predicate
    true_ugly_indices = np.where(y_pred != y)[0]
    accuracy_noise = calculate_accuracy(ugly_outlier_index, true_ugly_indices, len(data_copy))
    t_train = t_MDetO + t_MDetI + t_outlier + t_imbalanced + t_predicate
    print("è®­ç»ƒæ—¶é—´ä¸ºï¼š", t_train)
    return accuracy_noise

file_path = "./ugly_detection.log"
lock_obj = optuna.storages.journal.JournalFileOpenLock(file_path)

storage = optuna.storages.JournalStorage(
    optuna.storages.journal.JournalFileBackend(file_path, lock_obj=lock_obj),
)
# Execute an optimization by using an `Objective` instance.
study = optuna.create_study(storage=storage, direction="maximize")

t0 = time.time()  # å¼€å§‹æ—¶é—´
study.optimize(objective, n_trials=1)
t1 = time.time()  # ç»“æŸæ—¶é—´
print("è¶…å‚æ•°ä¼˜åŒ–è€—æ—¶(å«è®­ç»ƒæ—¶é—´)ï¼š", t1-t0)

trial = study.best_trial
print("Objective Values: {}".format(trial.value))
print("Best hyperparameters: {}".format(trial.params))


